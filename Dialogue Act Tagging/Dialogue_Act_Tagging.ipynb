{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "id": "jmTpKt_uefe5",
    "outputId": "860d0f6a-ed72-4486-9fed-2307105ba67c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "import sklearn.metrics\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {
    "id": "6E8axaw1hAbM"
   },
   "outputs": [],
   "source": [
    "f = glob.glob(\"swda/sw*/sw*.csv\")\n",
    "frames = []\n",
    "for i in range(0, len(f)):\n",
    "    frames.append(pd.read_csv(f[i]))\n",
    "\n",
    "result = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "b7hKGF7EhM4s",
    "outputId": "bbcb107d-00cd-4837-975f-8d86fb0d28ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of converations in the dataset: 223606\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of converations in the dataset:\",len(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of converations in the dataset: 223606"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ttyB2lQhc7B"
   },
   "source": [
    "The dataset has many different features, we are only using act_tag and text for this training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "id": "-jUifIdshhD0"
   },
   "outputs": [],
   "source": [
    "reduced_df = result[['act_tag','text']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-iPmZvysqg2i"
   },
   "source": [
    "Reduce down the number of tags to 43 - converting the combined tags to their generic classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "id": "MQuHm0jPt_lz"
   },
   "outputs": [],
   "source": [
    "# Imported from \"https://github.com/cgpotts/swda\"\n",
    "# Convert the combination tags to the generic 43 tags\n",
    "\n",
    "import re\n",
    "def damsl_act_tag(input):\n",
    "        \"\"\"\n",
    "        Seeks to duplicate the tag simplification described at the\n",
    "        Coders' Manual: http://www.stanford.edu/~jurafsky/ws97/manual.august1.html\n",
    "        \"\"\"\n",
    "        d_tags = []\n",
    "        tags = re.split(r\"\\s*[,;]\\s*\", input)\n",
    "        for tag in tags:\n",
    "            if tag in ('qy^d', 'qw^d', 'b^m'): pass\n",
    "            elif tag == 'nn^e': tag = 'ng'\n",
    "            elif tag == 'ny^e': tag = 'na'\n",
    "            else: \n",
    "                tag = re.sub(r'(.)\\^.*', r'\\1', tag)\n",
    "                tag = re.sub(r'[\\(\\)@*]', '', tag)            \n",
    "                if tag in ('qr', 'qy'):                         tag = 'qy'\n",
    "                elif tag in ('fe', 'ba'):                       tag = 'ba'\n",
    "                elif tag in ('oo', 'co', 'cc'):                 tag = 'oo_co_cc'\n",
    "                elif tag in ('fx', 'sv'):                       tag = 'sv'\n",
    "                elif tag in ('aap', 'am'):                      tag = 'aap_am'\n",
    "                elif tag in ('arp', 'nd'):                      tag = 'arp_nd'\n",
    "                elif tag in ('fo', 'o', 'fw', '\"', 'by', 'bc'): tag = 'fo_o_fw_\"_by_bc'            \n",
    "            d_tags.append(tag)\n",
    "        # Dan J says (p.c.) that it makes sense to take the first;\n",
    "        # there are only a handful of examples with 2 tags here.\n",
    "        return d_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "id": "S8N_PUCAblq3",
    "outputId": "eb927335-7bdc-48d5-8850-4be54efdf3dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-388-3a505b615f58>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reduced_df[\"act_tag\"] = reduced_df[\"act_tag\"].apply(lambda x: damsl_act_tag(x))\n"
     ]
    }
   ],
   "source": [
    "reduced_df[\"act_tag\"] = reduced_df[\"act_tag\"].apply(lambda x: damsl_act_tag(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UNy0vvhhqpD"
   },
   "source": [
    "There are 43 tags in this dataset. Some of the tags are Yes-No-Question('qy'), Statement-non-opinion('sd') and Statement-opinion('sv'). Tags information can be found here http://compprag.christopherpotts.net/swda.html#tags. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9biiyP8UiGDe"
   },
   "source": [
    "To get unique tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "id": "BrhW8gyLfQQK"
   },
   "outputs": [],
   "source": [
    "unique_tags = set()\n",
    "for tag in reduced_df['act_tag']:\n",
    "    unique_tags.add(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'%',\n",
       " '+',\n",
       " '^2',\n",
       " '^g',\n",
       " '^h',\n",
       " '^q',\n",
       " 'aa',\n",
       " 'aap_am',\n",
       " 'ad',\n",
       " 'ar',\n",
       " 'arp_nd',\n",
       " 'b',\n",
       " 'b^m',\n",
       " 'ba',\n",
       " 'bd',\n",
       " 'bf',\n",
       " 'bh',\n",
       " 'bk',\n",
       " 'br',\n",
       " 'fa',\n",
       " 'fc',\n",
       " 'fo_o_fw_\"_by_bc',\n",
       " 'fp',\n",
       " 'ft',\n",
       " 'h',\n",
       " 'na',\n",
       " 'ng',\n",
       " 'nn',\n",
       " 'no',\n",
       " 'ny',\n",
       " 'oo_co_cc',\n",
       " 'qh',\n",
       " 'qo',\n",
       " 'qrr',\n",
       " 'qw',\n",
       " 'qw^d',\n",
       " 'qy',\n",
       " 'qy^d',\n",
       " 'sd',\n",
       " 'sv',\n",
       " 't1',\n",
       " 't3',\n",
       " 'x'}"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "id": "LMOX5KwgiPmu"
   },
   "outputs": [],
   "source": [
    "one_hot_encoding_dic = pd.get_dummies(list(unique_tags))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encoding_dic[\"br\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "id": "ZPHPCxE3iPby"
   },
   "outputs": [],
   "source": [
    "tags_encoding = []\n",
    "for i in range(0, len(reduced_df)):\n",
    "    tags_encoding.append(one_hot_encoding_dic[reduced_df['act_tag'].iloc[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVI8QyVzjqWh"
   },
   "source": [
    "The tags are one hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQJTiffPjUtu"
   },
   "source": [
    "To create utterance representations as sequences of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "id": "PmkyD1TfjWGO"
   },
   "outputs": [],
   "source": [
    "utterances = []\n",
    "for i in range(0, len(reduced_df)):\n",
    "    utterances.append(reduced_df['text'].iloc[i].split(\" \"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "id": "MlD6L6e3jV-7"
   },
   "outputs": [],
   "source": [
    "wordvectors = {}\n",
    "index = 1\n",
    "for u in utterances:\n",
    "    for w in u:\n",
    "        if w not in wordvectors:\n",
    "            wordvectors[w] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "id": "e7_cjDHrjV1c"
   },
   "outputs": [],
   "source": [
    "# Max length of 137\n",
    "MAX_LENGTH = len(max(utterances, key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "id": "LX6DidEvjVWs"
   },
   "outputs": [],
   "source": [
    "utterance_embeddings = []\n",
    "for u in utterances:\n",
    "    utterance_emb = []\n",
    "    for w in u:\n",
    "        utterance_emb.append(wordvectors[w])\n",
    "    utterance_embeddings.append(utterance_emb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr4iEyNTjmlu"
   },
   "source": [
    "Then we split the dataset into test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "id": "GiNZ-iI_jnOF"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X_train, X_test, y_train, y_test = train_test_split(utterance_embeddings, np.array(tags_encoding))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_RqMeWe_jron"
   },
   "source": [
    "And pad the utterances with zero to make all utterances of equal length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "id": "yqD7DvzRGRY7"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "id": "Ai9cwv82jufe"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    " \n",
    "train_utterances_X = pad_sequences(X_train, maxlen=MAX_LENGTH, padding='post')\n",
    "test_utterances_X = pad_sequences(X_test, maxlen=MAX_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-AO0mle-fLO"
   },
   "source": [
    "Split Train into Train and Validation - about 10% into validation - In order to validate the model as it is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "id": "517zYSQLXkbn"
   },
   "outputs": [],
   "source": [
    "train_input = train_utterances_X[:140000]\n",
    "val_input = train_utterances_X[140000:]\n",
    "\n",
    "train_labels = y_train[:140000]\n",
    "val_labels = y_train[140000:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHJbZDtk7N-3"
   },
   "source": [
    "# Model 1 - BILSTM\n",
    "\n",
    "The first approach we'll try is to treat DA tagging as a standard multi-class text classification task, in the way you've done before with sentiment analysis and other tasks. Each utterance will be treated independently as a text to be classified with its DA tag label. This model has an architecture of:\n",
    "\n",
    "- Embedding  \n",
    "- BLSTM  \n",
    "- Fully Connected Layer\n",
    "- Softmax Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {
    "id": "M97Sw5iv-lEU"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(wordvectors) # 43,731\n",
    "MAX_LENGTH = len(max(utterances, key=len)) #137\n",
    "EMBED_SIZE = 100 # arbitary\n",
    "HIDDEN_SIZE = len(unique_tags) #43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "LCaX-ptaj8G2",
    "outputId": "df9036a6-c0d9-45e1-8ff0-2c1a283d16f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_60 (InputLayer)        [(None, 137)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_64 (Embedding)     (None, 137, 100)          4373200   \n",
      "_________________________________________________________________\n",
      "bidirectional_42 (Bidirectio (None, 137, 86)           49536     \n",
      "_________________________________________________________________\n",
      "bidirectional_43 (Bidirectio (None, 86)                44720     \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 43)                3741      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 43)                0         \n",
      "=================================================================\n",
      "Total params: 4,471,197\n",
      "Trainable params: 4,471,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input,Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "#Building the network\n",
    "\n",
    "# Include 2 BLSTM layers, in order to capture both the forward and backward hidden states\n",
    "\n",
    "# Embedding layer\n",
    "# Bidirectional 1\n",
    "# Bidirectional 2\n",
    "# Dense layer\n",
    "# Activation\n",
    "\n",
    "Input = Input(shape=(MAX_LENGTH,),dtype='int32')\n",
    "Embedding = Embedding(VOCAB_SIZE+1, EMBED_SIZE, input_length=MAX_LENGTH, mask_zero=True)(Input)\n",
    "Bidirectional_1 = Bidirectional(LSTM(HIDDEN_SIZE,return_sequences = True))(Embedding)\n",
    "Bidirectional_2 = Bidirectional(LSTM(HIDDEN_SIZE,return_sequences = False))(Bidirectional_1)\n",
    "Dense = Dense(HIDDEN_SIZE)(Bidirectional_2)\n",
    "Activation = Activation(\"softmax\")(Dense)\n",
    "model = Model(inputs=[Input], outputs=[Activation])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING:tensorflow:From /Users/mortro/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
    "# Instructions for updating:\n",
    "# Colocations handled automatically by placer.\n",
    "# Model: \"sequential_1\"\n",
    "# _________________________________________________________________\n",
    "# Layer (type)                 Output Shape              Param #   \n",
    "# =================================================================\n",
    "# embedding_1 (Embedding)      (None, 137, 100)          4371500   \n",
    "# _________________________________________________________________\n",
    "# bidirectional_1 (Bidirection (None, 137, 86)           49536     \n",
    "# _________________________________________________________________\n",
    "# bidirectional_2 (Bidirection (None, 86)                44720     \n",
    "# _________________________________________________________________\n",
    "# dense_1 (Dense)              (None, 43)                3741      \n",
    "# _________________________________________________________________\n",
    "# activation_1 (Activation)    (None, 43)                0         \n",
    "# =================================================================\n",
    "# Total params: 4,469,497\n",
    "# Trainable params: 4,469,497\n",
    "# Non-trainable params: 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140000\n",
      "27704\n",
      "140000\n",
      "27704\n"
     ]
    }
   ],
   "source": [
    "train_input = train_utterances_X[:140000]\n",
    "val_input = train_utterances_X[140000:]\n",
    "\n",
    "train_labels = y_train[:140000]\n",
    "val_labels = y_train[140000:]\n",
    "print(len(train_input))\n",
    "print(len(val_input))\n",
    "print(len(train_labels))\n",
    "print(len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 205
    },
    "id": "OeiLkgD3Arpl",
    "outputId": "dbf7ed58-571c-4ab2-b624-bcd98ed6eedf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "274/274 [==============================] - 1252s 5s/step - loss: 2.2923 - accuracy: 0.4454 - val_loss: 1.1967 - val_accuracy: 0.6562\n",
      "Epoch 2/3\n",
      "274/274 [==============================] - 1533s 6s/step - loss: 1.1186 - accuracy: 0.6795 - val_loss: 1.0300 - val_accuracy: 0.6952\n",
      "Epoch 3/3\n",
      "274/274 [==============================] - 1556s 6s/step - loss: 0.9411 - accuracy: 0.7265 - val_loss: 0.9895 - val_accuracy: 0.7029\n"
     ]
    }
   ],
   "source": [
    "# Train the model - using validation \n",
    "\n",
    "Training = model.fit(train_input,\n",
    "                    train_labels,\n",
    "                    epochs=3,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_input, val_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING:tensorflow:From /Users/mortro/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
    "# Instructions for updating:\n",
    "# Use tf.cast instead.\n",
    "# Train on 140000 samples, validate on 27704 samples\n",
    "# Epoch 1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "id": "2LkONUKQkSrL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 85s 152ms/step - loss: 1.0053 - accuracy: 0.7009\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_utterances_X, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "id": "Ab0ZL1dqkTY4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 70.09051442146301\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Accuracy:\", score[1]*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhMViQVSPY1J"
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "\n",
    "The overall accuracy is 67%, an effective accuracy for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHwoVCEwjEz7"
   },
   "source": [
    "In addition to overall accuracy, you need to look at the accuracy of some minority classes. Signal-non-understanding ('br') is a good indicator of \"other-repair\" or cases in which the other conversational participant attempts to repair the speaker's error. Summarize/reformulate ('bf') has been used in dialogue summarization. Report the accuracy for these classes and some frequent errors you notice the system makes in predicting them. What do you think the reasons areï¼Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H7owA1f27se8"
   },
   "source": [
    "## Minority Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "id": "UZ8BwgDxNcIr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23, 39, 23, ..., 39, 26, 23], dtype=int64)"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions for the test data\n",
    "predicted = model.predict(test_utterances_X,batch_size=100)\n",
    "pred_y_classes = predicted.argmax(axis=-1)\n",
    "pred_y_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  8, 23, ..., 26, 26, 23], dtype=int64)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "y_test_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_classes, pred_y_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "\n",
    "    ax = plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(\"Dialogue Act Confussion Matrix\")\n",
    "    plt.colorbar()\n",
    "    #tick_marks = np.arange(len(df_confusion.columns))\n",
    "    #plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    #plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(\"Groung Truth\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAR8AAADzCAYAAAC/phIAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3defRcZZ3n8fcnC2sSBBM4dIIdRjNKRE3Dz3Q0oxPBJS5D0NGZ0K3giIKcoLjMacE+c1ymcwZPI7Rgw3RYhqVVSKN00yCbIIO0MZAgEAIypgnCT2hClCUsQpbv/PE8RW5VarlVdavuUt/XOXV+dZ+q+9zn90vVN8997n2+j8wM55wbtgl5N8A5N5o8+DjncuHBxzmXCw8+zrlcePBxzuXCg49zLhcefJwrKEnWxeP6vNvbrUl5N8A515qkVO8zs+kDbkrmPPg4V2BdBJ8BtyR7HnycK7C0waeMPPg4V1CSmDAh3bDs9u3bB9ya7Hnwca7AqtzzKcXVLkmLJT0oaYOkUzOq82FJ6yTdLWlNj3VcJGmTpPsSZftJuknSr+PPfTOo8+uSfhvberekD3RR30GSfirpAUnrJZ3Sbzvb1NlPO/eQdIeke2Kd38igna3q7Lmdcf+Jkn4p6Zp+25jiWKkeZVT44CNpIvC3wPuBucAxkuZmVP27zGyemY31uP/FwOKGslOBm81sDnBz3O63ToCzYlvnmdmPu6hvG/BlMzsEWAAsi3+/ftrZqs5+2vkScISZvQWYByyWtKDPdraqs592ApwCPJDY7vffvCUPPvmaD2wws4fM7GXgcmBJzm0CwMxuA37fULwEuCQ+vwQ4OoM6e2Zmj5vZXfH5FsKXZmY/7WxTZz/tNDN7Lm5Ojg/rs52t6uyZpFnAB4ELEsV9/Zt3OJ4HnxzNBB5NbI/T5wc9MuBGSWslnZBBfTUHmNnjEL6kwP4Z1XuypHvjaVlP3XpJs4E/AVZn1c6GOvtqZzyduRvYBNxkZn23s0Wd/bTzb4C/AHYkygbyb5428HjwGZxmf9ksbmpYaGaHEU7nlkl6ZwZ1Dsp5wGsJpw6PA9/utgJJU4AfAl8ws2ezaFSTOvtqp5ltN7N5wCxgvqRD+21jizp7aqekDwGbzGxtv+1Ky4NPvsaBgxLbs4DH+q3UzB6LPzcBVxFO77LwhKQDAeLPTf1WaGZPxC/RDuB8umyrpMmEIPE9M/tRFu1sVme/7awxs6eBWwljX5n8PZN19tHOhcBRkh4mnP4fIenvs2pjMxMmTEj1KKMytPpOYI6kgyXtBiwFru6nQkl7S5paew68F7iv/V6pXQ0cF58fB/xTvxXWPtjRh+mirQr/LV4IPGBmZ2bRzlZ19tnOGZJeFZ/vCbwb+FWf7WxaZ6/tNLPTzGyWmc0mfA5vMbOP99PGTqrc8yn8fT5mtk3SycANwETgIjNb32e1BwBXxX+0ScD3zazriXmSfgAsAqZLGge+BpwOrJR0PPAI8LEM6lwkaR7hdPNh4MQuqlwIfAJYF8c+AL7aZztb1XlMH+08ELgkXt2cAKw0s2skreqjna3qvKyPdjbT1795K2UOLGmojHNCnBsFkyZNsqlTp6Z679NPP722j1tGclH4no9zo6zKPR8PPs4VWJWDTxkGnJ0bWVkNOKvJtJ3Ea/9dISHZ9ETZaQrTmR6U9L5E+eEK05I2SDo7XnxA0u6SrojlqxXu/2rLg49zBaU4qz2jS+0X02TajqSDgPcQBsprZXMJV/PeGPc5Nw7aQ7hH6gRgTnzU6jweeMrMXgecBXyrU4NKFXyU7Z3IpamzDG0sS51laGND3Zn0fNpM2zmLcMd28srTEuByM3vJzDYCGwg3aB4ITDOzVRauVF3KzqkkySkmVwJHqkPDShV8CBF3FOssQxvLUmcZ2viKLoLPdElrEo+ObZJ0FPBbM7un4aVWU5pmxueN5XX7mNk24Bng1e2On8uAs6TFwHcI9+1cYGan59EO54quiwHnzd1cape0F/CXhBtsd3m5SZm1KW+3T0tDDz7amSLjPYTIeaekq83s/lb7TJ8+3WbPns1rXvMaxsbGMr0xqQx1lqGNZakz7zauXbt2s5nNSPPeAd9k+FrgYOCeeIxZwF2S5tN6StN4fN5YTmKfcUmTgH3okJ0hj57PKykyACTVUmS0DD6zZ89mzZqe8n05VyiSftPl+wfSDjNbR2L2vcJ8tTEz2yzpauD7ks4E/ogwsHyHmW2XtEUhJ9Jq4FjgnFhFbYrJKuCjhKknbQNyHmM+qVJkSDqhdv765JNPDq1xzhVJhpfaf0AIDK+XNB6ngjQVpy+tJHQIrgeWmVktSfRJhFxGG4B/Ba6L5RcCr5a0AfgSKRKq5dHzSXVuaGYrgBVA5t1k58oiqxnrZnZMh9dnN2wvB5Y3ed8aYJdUJ2b2B7qc05ZH8BlIigznqqbqE0vzOO3KPEWGc1WV1WlXEQ295zOgFBnOVVJZA0saudznE1cL6HbFAOdGjgcf51wuPPg454auNrG0qjz4OFdg3vNxzuXCg49zLhcefJxzQ1fme3jS8ODjXIF58HHO5cKDj2PHjh1121W+BOqKo8qfMw8+zhWUj/k453LjwSdjMWvaFmA7sK1sy7w6NywefAbjXWa2Ocfjd6XK596uuDz4OOdyUeXgk9d/5wbcKGntIBdcc67M0iYSK2uAyqvns9DMHpO0P3CTpF/FFRVfEYPSCRCWJnFuFFX5dD+X38zMHos/NwFXEZbTaXzPCjMbM7OxGTNSLXPkCsLM6h6udxmuXnGRpE2S7kuU/bWkX0m6V9JVkl6VeO00SRskPSjpfYnywyWti6+dXVsSWdLukq6I5aslze7UpqEHH0l7S5pae05YMfG+9ns5N5oyPO26GFjcUHYTcKiZvRn4f8Bp8ZhzCbnV3xj3OVdhsU+A8whnJHPio1bn8cBTZvY6wvrv3+rUoDx6PgcAt0u6B7gDuNbMrs+hHc4VWpZjPnFY4/cNZTfGddUBfsHO1UiXAJeb2UtmtpGwRtd8SQcC08xsVVwQ8FLg6MQ+l8TnVwJHqkPD8kgg/xDwlmEf17ky6mIwebqk5LK+K+Lad2l9CrgiPp9JCEY1tYU9t8bnjeW1fR6FVxaJeAZ4NdDydhq/1O4yV9arL0XUxd9yc68360r6S2Ab8L1aUZO3WZvydvu05MHHuQIbdCCXdBzwIeDIxNrqrRb2HGfnqVmyPLnPuKRJwD40nOY1qu51POdKrpZAPs2jx/oXA18BjjKzFxIvXQ0sjVewDiYMLN9hZo8DWyQtiOM5xwL/lNjnuPj8o8At1uFSp/d8nCuwrHo+kn4ALCKMDY0DXyNc3dqdcK8dwC/M7LNmtl7SSuB+wunYMjPbHqs6iXDlbE/guvgAuBC4TNIGQo9naac2efBJqfFDMIj7VxrrLOvYyZYtW+q2p06d2nUdW7durduePHlyX21qptscTY888kjd9jBufs3qM2BmxzQpvrDN+5cDy5uUrwEObVL+B+Bj3bTJg49zBVbW/4DS8ODjXIF58HHODV2ZJ42m4cEnpWHMUarKB62XMZ5GgxjjadTtVaI8JjhX5TPRjAcf5wqsyrPaPfg4V2De83HODV3Vx3wG1qdrkT9kP0k3Sfp1/LnvoI7vXBVUOZPhIE8oL2bX/CGnAjeb2Rzg5rhdCmkSZG3durXuURWeHCw/Hnx60Cx/CPU5Py5hZy4Q51wTVQ4+wx7zOSBOTsPMHlfI4eyca6I2sbSqCjvgLE8g71xpezVpDDusPhFTMRJ/bmr1xjImkJ88eXLdoyqq0MUvqyqfdg07+CRzfhzHzlwgzrkmqhx8Bnba1SJ/yOnASknHA4/Q5RR850ZNWQNLGgMLPi3yhwAcOahjOlclZe7VpFHYAeeiqfKHwBVXlT93HnycKzC/1O6cGzo/7XLO5abKwae6fboS2LZtW93DuUZZXWrvdqK3pNMkbZD0oKT3JcoPl7QuvnZ2XEIHhWV2rojlqyXN7tQmDz7OFViG9/lcTMqJ3pLmEpa+eWPc51xJE+M+5xFmHsyJj1qdxwNPmdnrgLOAb3VqkAcf5wosq+DT5UTvJcDlZvaSmW0ENgDz46yEaWa2Ki4IeGnDPrW6rgSOVIeG+ZiPcwXV5YDzdElrEtsrzGxFh31aTfSeCfwi8b7xWLY1Pm8sr+3zaKxrm6RngFcDm1sd3INPSt0uMJfGpEn1f/7nn3++bnvvvffu+xhF0CwHUBkHUvNY1LGLz9lmMxvL6LDNfjFrU95un5ZSBR9JM4E/Tr4/duOccwM04AD3hKQDY68nOdF7HDgo8b5ZwGOxfFaT8uQ+45ImAfuw62lenY7BR9K3gP9KWLe5tl6zAR58nBuwAQef2kTv06mf6H018H1JZwJ/RBhYvsPMtkvaImkBsBo4Fjinoa5VwEeBW6xD2ss0PZ+jgdeb2Utd/VrOub5keZNhNxO9zWy9pJWEDsc2YJmZ1ToeJxGunO0JXBcfENZ9v0zSBkKPZ2mnNqUJPg8Bk4Gugo+ki4APAZvM7NBY9nXgM8CT8W1fNbMfd1NvXoZxfl+VMZ5GZRzfaSaP3yOrY3Y70dvMlgPLm5SvAQ5tUv4HusxS0TL4SDqHcHr1AnC3pJtJBCAz+3yHui8Gvku4HJd0lpmd0U0jnRtVVQnczbTr+dQu260lnM8ldVzCwMxuS3OXo3OutZEMPmZ2CYCkU8zsO8nXJJ3SxzFPlnQsIbh92cyeavYmeQ5nN+JU8QTyaX6z45qUfbLH450HvBaYBzwOfLvVG8uYw9m5rI1kGlVJxwB/BhwsKXnaNRX4XS8HM7MnEvWfD1zTSz3OjYqyBpY02o35/JzQO5lOfQ9lC3BvLwer3dAUNz8M3Nfu/c6NupEMPmb2G+A3wNt6qbjFfQWLJM0jDFg/DJzYS93OjYqRDD41kraw8+rWboR7fp43s2nt9mtxX8GFXbfQuRFV5vGcNDoGHzObmtyWdDQwf2AtKqgqfwhccVX5c9f1dTwz+0fgiAG0xTnXYMKECakeZZTmtOsjic0JwBgpbjJ0zvWvyj2fNHO7/lPi+TbCQPGSgbTGOfeKkR7ziXlb7zWzs4bUnsIaRDIxVy55JBOrcvBp+w2K0+iPGlJbnHMNRvIO54SfS/oucAXwSp5PM7trYK1yzgHV7vm0m15xo5m9F3h7LPpm4mXDr3g5N1BVn1jaruczA8DM3jWkthRalT8ELp0yJxMronbBZ5+Gy+x1zOxHA2iPcy5hZIMPIQ1qqyUxPPg4N2CjGnx+Y2afGlpLnHO7qHLwaTeQ0ddvLekgST+V9ICk9bXsh2qzOH2RmVndo6y2b99e93DFlfYye5oAJemL8Xt4n6QfSNqj3XdR0mmSNkh6UNL7EuWHS1oXXztbfUTHdsHnE71WGm0jpEk9BFgALFNYgL7p4vTOuV1lEXwUFv38PDAWV5KZSFjapul3MX5PlwJvBBYD58YbjiFkIz2BsJbXnPh6T1oGHzPrK9GXmT1euxfIzLYADxDWc261OL1zrkGGE0snAXsqrCa6F2Gl0VbfxSXA5Wb2kpltBDYA8xVWNZ1mZqvigoCX0sf3dyjXj+MqFn9CWOWwbnF6YP8W+5wgaY2kNU8++WSztzhXeV30fKbXvi/xcUKtDjP7LXAGYWHAx4FnzOxGWn8XZwKPJpoxHstmxueN5T1JtVZ7PyRNAX4IfMHMnk17imhmK4AVAGNjY7kPslRl4G/ixImd35SxZmNkVfl7DlKXUyc2m9lYi3r2JfRmDgaeBv5B0sfbHbpJmbUp70malBrrmhzgGcLSN39lZi2TyUuaTAg830vcF9RqcXrnXIOMgvS7gY1m9mSs80eEmQutvovjwEGJ/WcRTtPG4/PG8p6kOe26DrgW+PP4+GfgNuDfCKuSNhVHwS8EHjCzMxMv1RaUh/rF6Z1zDTK62vUIsEDSXvF7eSRhDLbVd/FqYKmk3SUdTBhYviOemm2RtCDWcyx9fH/TnHYtNLOFie11kv7FzBZ26LotJFwxWyfp7lj2VVosTu+c21UWPR8zWy3pSuAuwlXoXxKGNKbQ5LtoZuslrQTuj+9fFjNcAJxE6HTsSeiYXNdru9IEnymS/tTMVgNImh8bTWxYU2Z2O63vFWq6OL1zrl5WY2Nm9jXCCjJJL9Hiu2hmy4HlTcrXAIdm0aY0wefTwEVx4FjAs8CnJe0N/K8sGuGqzQeXe6MRntUOgJndCbxJ0j6AzOzpxMsrB9Yy51ylA3eaq127A/8ZmA1Mqv0xzOybbXZzzmVgpIMPYTT7GWAt4RzROTckox58ZplZz/M3XPmdfPLJddvnnHNO3fYgviDDSNa+devWuu3Jkydnfox+dHmTYemkGc36uaQ3DbwlzrldZDWrvYjS9Hz+A/BJSRsJp10CzMzePNCWOedKG1jSSBN83j/wVjjnmhrpS+340sgj7+yzz67bHsZ4zDD+xy/aGE+jMp9SpZEm+FzLzhmtexBmxj5ISDTknBugkQ4+ZlY32CzpMODEgbXIOfeKKgefrk8oY3bCt3Z6n1rncP66pN9Kujs+PtBDu50bCSN9tUvSlxKbE4DDgDSpBWs5nO+SNBVYK+mm+NpZZnZG1611uajyoGfRlTWwpJFmzGdq4vk2whjQDzvtFHN/1FI0bpFUy+HsnEuhzL2aNNKM+XwDIPZezMye6/YgDTmcFwInSzqWkA3xy2b2VLd1OjcKqtzr7PibSTpU0i+B+4D1ktZKSp3PozGHM2HpjdcC8wg9o2+32M8TyLuRN9JjPoSMZ18ys58CSFoUy97eacdmOZzN7InE6+cD1zTbt2gJ5F944YW67b322iunlgxft/f1rFq1qm77bW9728CPOQyNiywOIxl/EX7vQUnTp9u7FngAzOxWYO9OO7XK4RwTVdd8mNCjcs41SNvrKWuAStPzeUjS/wAui9sfBzam2K9VDudjJM0j3Lj4MH7PkHMtlTWwpJEm+HwK+AZQW/rmNuC/ddqpTQ7nH6dunXMjbmSDj8L6zP9gZu8eUnsKaxhjPEUc54Du29E4xtOYNwc6z6sqyu+elMeCi1le7ZL0KuACQgJ4I3QsHgSuIGQqfRj4L7Wrz5JOA44HtgOfN7MbYvnh7FzB4sfAKdZsZcgO2v5mcbmMF2L+ZufcEA1gzOc7wPVm9gbgLYS1u04FbjazOcDNcRtJc4GlhDmci4FzY2cEwhXrEwjrec2Jr3ctzWnXHwjjNjcBz9cKzezzvRzQOZdeVj1ASdOAdwKfBDCzl4GXJS0BFsW3XQLcCnyFsLzy5Wb2ErBR0gZgvqSHgWlmtirWeylwND2s35V2Vvu13VbsnOtfF8FnuqQ1ie0V8XaVmn9HmBb1fyS9hZCT/RTggDgbgbhs8v7x/TOBXyT2H49lW+PzxvKupbnD+ZJeKnbO9a+L4LPZzMbavD6JMC/zc3EF0+8QT7FaHbpJmbUp71rLMR9JSyQtS2yvlvRQfHy0l4O59op678aLL75Y9+jW5MmTd3kUwY4dO+oeRZThmM84MF5beRi4khCMnqjdexd/bkq8/6DE/rOAx2L5rCblXWs34PwXhAXja3YnpNJYRFiv2Tk3QFkOOJvZvwGPSnp9LDqSsBb71cBxsew4wlJZxPKlknaXdDBhYPmOeIq2RdKCeCPxsYl9utLutGs3M3s0sX27mf0O+J3CUsnOuQHLeGLp54DvSdoNeIhwv94EYKWk44FHgI8BmNl6SSsJAWobsCxe/YbQ+biYcKn9OnoYbIb2wWff5IaZJRdvmtHLwZxz3cny9NvM7gaajQsd2eL9y4HlTcrXEO4V6ku74LNa0mfM7PxkoaQTgTv6PbArjz322GPgx3juufpMLVOmTBn4Mbu9L27YN4EWbewva+2CzxeBf5T0Z8BdsexwwtjP0YNumHOumHd6Z6Vl8DGzTcDbJR3BzpUqrjWzW4bSMufcaAafmhhsug44kvYgTELdPR7nSjP7mqT9aDGXxDlXb6SDTx9eAo4ws+diUrHbJV0HfIQwl+R0SacSbnT6ygDbkYlt27bVbU+aNMg/XbE0JtHq9Lv3MjbSOMazZs2auu2xsXb3z/Wm24mieQSCKgefgSWItaA2ijg5PowwZ6R21/Ql+PiRc01JYsKECakeZTTQVkuaGBOJbQJuindX1s0lAfZvV4dzo6zKmQwHGnzMbLuZzSPcgj1f3SWe9wTybuRVOfgMZeDCzJ6WdCsh78cTkg6MM2iTc0ka9ylUAvlRGuNp1O3vnsWXYRBjPGVU1sCSxsB6PpJmxMxpSNoTeDfwK1rPJXHOJQwgmVihDPK/8wOBS2L2swnASjO7RtIqmswlcc7tqqyBJY2BBR8zu5ewSmlj+e9oMZfEOVfPg49zLhdlvYyehgcf5wqqzOM5aXjwca7APPg453Lhwce5gmucfwb5LPKXNQ8+zrlcePBxzg2dDzg753JT5Uvt1f3NnKuALKdXxCwTv5R0TdzeT9JNkn4df+6beO9pkjZIelDS+xLlh0taF187W310zTz4uEqYOHHiLo8qyHhu1ynAA4ntUwmJ/eYAN8dtJM0FlhLSJy8Gzo3TpADOA04grOM1J77eEw8+zhVUlhNLJc0CPghckChuldhvCXC5mb1kZhuBDYSUOAcC08xslYV0lZfSRzJAH/NxrsC66NVMl5TMPbsipqWp+RvCKsRTE2V1if0k1RL7zQR+kXjfeCzbGp83lvdkYMGnTQL5rwOfAWoZwr5qZj8eVDucK7Mugs9mM2uaBEnSh4BNZrZW0qI0h21SZm3Ke5JHAnmAs8zsjAEe27lKyOhq10LgKEkfAPYApkn6e1on9hsHDkrsPwt4LJbPalLekzwSyDvnUshqzMfMTjOzWWY2mzCQfIuZfZzWif2uBpZK2l3SwYSB5TviKdoWSQviVa5j6SMZYB4J5AFOlnSvpIuSl/ca9vUczm7kDTiT4enAeyT9GnhP3MbM1gMrgfuB64FlZlabv3ISYdB6A/CvwHWNlab+3bpdr7qng4R0qlcBnyOM9Wwm9IL+J3CgmX2q3f5jY2PWuI6Tc2UkaW2rsZlGb3jDG+yCCy7o/EbgHe94R+p6i2LoCeSTYz2SzgeuGUYb+rVjx4667SzOxQdR5yA09jxnzJiRU0uy9dRT9Qvl7rtv0054rqo8vWLoCeTjwFbNh4H7BtUG58puwKdducojgfxlkuYRTrseBk4cYBucK60yB5Y08kgg/4lBHdO5qinqqXgW/A7nlAbxISjLB6sqYzyNGsd4nn322brtadOmDbM5TXnPxzmXCw8+zrmh8zEf51xuPPg4NwKKMMbTyIOPcy4XHnycc0MnqTRXRHvhwce5AvOej+P++++v2547d27mx2ic5FvWD97LL79ct73bbrt13KfTBOdB/C26/XvfeeedddtvfetbM29To7J+BtLw4ONcgXnwcc7lwoOPc27oqn6T4cCH0tXFQmXOuXqeUqM/tYXKandw1RYqO13SqXH7K0NoR18OOeSQgR+jrB+iRmkGmBvl8bt3e8xhDDA3qvKl9kHncO5moTLnXIMq93wGHVZrC5Ul84XWLVQG7N9sR08g70ZdVqtXxLoOkvRTSQ9IWi/plFie23rtg0yj+spCZb3sb2YrzGzMzMaqmk/GuU4y7PlsA75sZocAC4BlCmuy57Ze+yB7PrWFyh4GLgeOUGKhMgDVL1RWaFXo5rryySr4mNnjZnZXfL6FMA47kxzXax/kooHdLlTmnGswiDEfSbMJKY5X03oYZCbwaGK32rrsM8lovfY87vM5HVgp6XjgEeBjObTBuVLoIrBMl5Rc3G6Fma1oUt8U4IfAF8zs2Tb1D3y99mGt23UrcGt8/jvgyGEc17ky63JW++ZOiwZKmkwIPN8zsx/F4tzWa6/uTQTOVUCGV7sEXAg8YGZnJl7Kbb12n17hXIFleHFjIfAJYJ2ku2PZV2kxDGJm6yXV1mvfxq7rtV8M7ElYq72n9do9+DhXYFkFHzO7nebjNdBiGMTMlgPLm5SvAQ7tt00efJwrqKrf1lGK4LN27drNkn4DTAc2Z1x9GeosQxvLUmfebfzjbir24JMzM5sBIGlNpxH9bpWhzjK0sSx1lqGNDXUPotpCKEXwcW5UVXlWuwcf5wrKx3yKZZc7NkekzjK0sSx1lqGNr6hy8FGnVQNcsUjaDqwj/MfxAHCcmb3QY10XA9eY2ZWSLgDONLP7W7x3EfCymf28y2M8DIyZWdaDvJU3b948+8lPfpLqvTNmzFg7qHGnQanuCWV1vWhm88zsUOBl4LPJFxNpD7piZp9uFXiiRcDbe6nb9W4QE0uLwoNPuf0MeJ2kRTFR1PcJd7BOlPTXku6UdK+kEyHcYi/pu5Lul3QtiURukm6VNBafL5Z0l6R7JN0cZ0F/FviipLslvUPSDEk/jMe4U9LCuO+rJd2okLf772h9Y5tLocrBp2xjPi6SNAl4P3B9LJoPHGpmGyWdADxjZm+VtDvwL5JuJKRReD3wJuAAwq3zFzXUOwM4H3hnrGs/M/u9pP8NPGdmZ8T3fR84y8xul/Qa4AbgEOBrwO1m9k1JHyQknXI96HJiael48CmfPRNzc35GmCz4dsKkv42x/L3AmyV9NG7vQ5gY+E7gB3GOzmOSbmlS/wLgtlpdZvb7Fu14NzA38b/uNElT4zE+Eve9VtJTPf6ejmoPOHvwKZ8XzWxesiB+QJ9PFgGfM7MbGt73ATrnXlGK90A4ZX+bmb3YpC1+FSMjVQ4+1e3TjbYbgJNi/hYk/XtJewO3EdIkTIy5W97VZN9VwH+MaRSQtF8s3wJMTbzvRuDk2oakWkC8DfjzWPZ+wNdl61Ha8Z6yBigPPtV0AWE85y5J9wF/R+jlXgX8mnCp/jzg/zbuaGZPEsZpfiTpHuCK+NI/Ax+uDTgDnwfG4oD2/ey86vYN4J2S7iKc/j0yoN9xJFQ5+Ph9Ps4V1GGHHWY/+9nPUr13ypQppbvPx8d8nCuwsvZq0vDg41xB+aV251xuvOfjnMuFBx/nXC6qHHyqe0LpXAVkdak9ztd7UNIGSacOoekdefBxrqCyuslQIdPB3xLmAs4FjpE0dwi/QlsefJwrsIx6PvOBDWb2kEWzUyoAAAC1SURBVJm9DFwOLBl44zvwMR/nCiyjS+0zgUcT2+PAn2ZRcT88+DhXUGvXrr1B0vSUb99D0prE9gozq6V3bdY1yn1qgwcf5wrKzBZnVNU4cFBiexbwWEZ198zHfJyrvjuBOZIOlrQbsBS4Ouc2ec/Huaozs22STiakWpkIXGRm63Nuls9qd87lw0+7nHO58ODjnMuFBx/nXC48+DjncuHBxzmXCw8+zrlcePBxzuXCg49zLhf/H+M1/irPUwKLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mat = plot_confusion_matrix(cm)\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "id": "muWtF2t0W0zd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br accuracy: 0.0\n",
      "br accuracy: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate Accuracies for \"br\" = 18 and \"bf\"= 15\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "mat = confusion_matrix(y_test_classes, pred_y_classes) \n",
    "classes_accuracy = mat.diagonal()/mat.sum(axis=1)   #Calculates the accuracy of each individual classes\n",
    "\n",
    "\n",
    "bf_acc = classes_accuracy[15]   #bf class at index 15\n",
    "print(\"br accuracy:\", bf_acc)\n",
    "\n",
    "br_acc = classes_accuracy[18]   #br class at index 18\n",
    "print(\"br accuracy:\", br_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdnpWLggZ-6z"
   },
   "source": [
    "\n",
    "Due to the reduced lack of training data for the minority classes, these minority classifiers will not be very confident in classification, as they have not been fully optimised. The frequent classifiers will be more optimised and will generate more confident scores for all examples, effectively crowding out the less confident minority classifiers. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZ16sE5F7x9e"
   },
   "source": [
    "# Model 2 - Balanced Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKHbOs4WkFaP"
   },
   "source": [
    "\n",
    "One thing we can do to try to improve performance is therefore to balance the data more sensibly. As the dataset is highly imbalanced, we can simply weight the loss function in training, to weight up the minority classes proportionally to their underrepresentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "6L4kNdf6kGEa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\FiercePC\\anaconda3a\\lib\\site-packages\\sklearn\\utils\\validation.py:67: FutureWarning: Pass classes=[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42], y=[11 27 18 ...  8 39 39] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y_integers = np.argmax(tags_encoding, axis=1)\n",
    "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
    "d_class_weights = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 50.48679160081282,\n",
       " 1: 50.00134168157424,\n",
       " 2: 24.762569213732004,\n",
       " 3: 44.44563705028821,\n",
       " 4: 5.273975187508845,\n",
       " 5: 17.16217668278456,\n",
       " 6: 7.123478814909206,\n",
       " 7: 15.072868217054264,\n",
       " 8: 0.06896462388610163,\n",
       " 9: 18.182306066026996,\n",
       " 10: 2.011659394539157,\n",
       " 11: 4.2554333346020625,\n",
       " 12: 25.121447028423773,\n",
       " 13: 3.7736861646471125,\n",
       " 14: 1.0924662888411178,\n",
       " 15: 6.915079168728353,\n",
       " 16: 6.1106222501571335,\n",
       " 17: 47.273995771670194,\n",
       " 18: 4.272916626855975,\n",
       " 19: 49.52513842746401,\n",
       " 20: 4.23809253046758,\n",
       " 21: 7.914976461010229,\n",
       " 22: 8.996781202220971,\n",
       " 23: 0.13548733839357288,\n",
       " 24: 3.972604686694974,\n",
       " 25: 55.91547886971743,\n",
       " 26: 0.27821622892749026,\n",
       " 27: 2.613135444665186,\n",
       " 28: 1.7105722154222767,\n",
       " 29: 1.0840399280558102,\n",
       " 30: 0.33159925614613706,\n",
       " 31: 9.33597762097616,\n",
       " 32: 17.391771019677996,\n",
       " 33: 4.91971573782755,\n",
       " 34: 65.00174418604651,\n",
       " 35: 65.82455107447748,\n",
       " 36: 0.4660876162842808,\n",
       " 37: 1.4138497919749105,\n",
       " 38: 66.66845557543232,\n",
       " 39: 0.19603194989571837,\n",
       " 40: 5.411175374488783,\n",
       " 41: 7.547372329294225,\n",
       " 42: 23.11173126614987}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zF1UM-ZMZoa1"
   },
   "source": [
    "## Define & Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "id": "xIRgRAzOPSAZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_54 (InputLayer)        [(None, 137)]             0         \n",
      "_________________________________________________________________\n",
      "embedding_49 (Embedding)     (None, 137, 100)          4373200   \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 137, 86)           49536     \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 86)                44720     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 43)                3741      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 43)                0         \n",
      "=================================================================\n",
      "Total params: 4,471,197\n",
      "Trainable params: 4,471,197\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Re-built the model for the balanced training\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input,Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "Input = Input(shape=(MAX_LENGTH,),dtype='int32')\n",
    "Embedding = Embedding(VOCAB_SIZE+1, EMBED_SIZE, input_length=MAX_LENGTH, mask_zero=True)(Input)\n",
    "Bidirectional_1 = Bidirectional(LSTM(HIDDEN_SIZE,return_sequences = True))(Embedding)\n",
    "Bidirectional_2 = Bidirectional(LSTM(HIDDEN_SIZE,return_sequences = False))(Bidirectional_1)\n",
    "Dense = Dense(HIDDEN_SIZE)(Bidirectional_2)\n",
    "Activation = Activation(\"softmax\")(Dense)\n",
    "model2 = Model(inputs=[Input], outputs=[Activation])\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "xB2McUREkL4B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "274/274 [==============================] - 788s 3s/step - loss: 3.4769 - accuracy: 0.1829 - val_loss: 2.2969 - val_accuracy: 0.3370\n",
      "Epoch 2/3\n",
      "274/274 [==============================] - 963s 4s/step - loss: 2.2051 - accuracy: 0.3671 - val_loss: 2.0520 - val_accuracy: 0.3942\n",
      "Epoch 3/3\n",
      "274/274 [==============================] - 1036s 4s/step - loss: 1.6970 - accuracy: 0.4330 - val_loss: 2.0157 - val_accuracy: 0.4125\n"
     ]
    }
   ],
   "source": [
    "# Train the balanced network -  takes  time to achieve good accuracy\n",
    "Training = model2.fit(train_input,\n",
    "                    train_labels,\n",
    "                    epochs=3,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_input, val_labels),\n",
    "                    verbose=1,class_weight=d_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJPjlMclZtw2"
   },
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "8UMAMGpJRINC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 67s 120ms/step - loss: 2.0077 - accuracy: 0.4171\n"
     ]
    }
   ],
   "source": [
    "# Overall Accuracy\n",
    "score = model2.evaluate(test_utterances_X, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "0xzLIkTarjei"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 41.710492968559265\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Accuracy:\", score[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "id": "qkULcz2igEW3"
   },
   "outputs": [],
   "source": [
    "# Generate predictions for the test data\n",
    "label_pred = model2.predict(test_utterances_X, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hq7i7giWZ4_l"
   },
   "source": [
    "## Balanced network evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM7VWweco0Et"
   },
   "source": [
    "Report the overall accuracy and the accuracy of  'br' and 'bf'  classes. Suggest other ways to handle imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "4jNfWmSNgRvT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "br accuracy: 28.723404255319153\n",
      "br accuracy: 3.4013605442176873\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the confusion matrix off these predictions\n",
    "\n",
    "matrix_balanced = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1))\n",
    "\n",
    "mat = plot_confusion_matrix(matrix_balanced)\n",
    "mat\n",
    "\n",
    "# Calculate Accuracies for \"br\" and \"bf\"\n",
    "\n",
    "mat = confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1)) \n",
    "classes_accuracy = mat.diagonal()/mat.sum(axis=1)   #Calculates the accuracy of each individual classes\n",
    "\n",
    "\n",
    "bf_acc = classes_accuracy[15]   #bf class at index 15\n",
    "print(\"bf accuracy:\", bf_acc*100)\n",
    "\n",
    "br_acc = classes_accuracy[18]   #br class at index 18\n",
    "print(\"br accuracy:\", br_acc*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfrGWuZ6nk4y"
   },
   "source": [
    "# Using Context for Dialog Act Classification\n",
    "\n",
    "The second approach we will try is a hierarchical approach to DA tagging. We expect there is valuable sequential information among the DA tags. So in this section we apply a BiLSTM on top of the utterance CNN representation. The CNN model learns textual information in each utterance for DA classification, acting like the text classifier from Model 1 above. Then we use a bidirectional-LSTM (BLSTM) above that to learn how to use the context before and after the current utterance to improve the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qyPpNaK-2mb"
   },
   "source": [
    "## Define the model\n",
    "\n",
    "This model has an architecture of:\n",
    "\n",
    "- Word Embedding\n",
    "- CNN\n",
    "- Bidirectional LSTM\n",
    "- Fully-Connected output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DuJLqgjWqcIf"
   },
   "source": [
    "## CNN\n",
    "\n",
    "\n",
    "This is a classical CNN layer used to convolve over embedings tensor and gether useful information from it. The data is represented by hierarchy of features, which can be modelled using a CNN. We transform/reshape conv output to 2d matrix. Then we pass it to the max pooling layer that applies the max pool operation on windows of different sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "id": "XA5INtFl-fM0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 1, 43)\n",
      "(None, 1, 43)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.layers import Dense, Reshape,BatchNormalization,MaxPool2D\n",
    "from keras.layers import Input,Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding,Conv2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 64\n",
    "drop = 0.2\n",
    "VOCAB_SIZE = len(wordvectors) # 43,731\n",
    "MAX_LENGTH = len(max(utterances, key=len)) #137\n",
    "EMBED_SIZE = 100 # arbitary\n",
    "HIDDEN_SIZE = len(unique_tags) \n",
    "\n",
    "# CNN model\n",
    "Input_1 = Input(shape=(MAX_LENGTH,),dtype='int32')\n",
    "embedding = Embedding(input_dim=VOCAB_SIZE+1, output_dim=EMBED_SIZE, input_length=MAX_LENGTH)(Input_1)\n",
    "reshape = Reshape((MAX_LENGTH, EMBED_SIZE, 1))(embedding)\n",
    "\n",
    "# 3 convolutions\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "bn_0 = BatchNormalization()(conv_0)\n",
    "\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "bn_1 = BatchNormalization()(conv_1)\n",
    "\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], EMBED_SIZE), strides=1, padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "bn_2 = BatchNormalization()(conv_2)\n",
    "\n",
    "# maxpool for 3 layers\n",
    "maxpool_0 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[0] + 1, 1), padding='valid')(bn_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[1] + 1, 1), padding='valid')(bn_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(MAX_LENGTH - filter_sizes[2] + 1, 1), padding='valid')(bn_2)\n",
    "\n",
    "\n",
    "# concatenate tensors\n",
    "concat_tensors = Concatenate()([maxpool_0,maxpool_1,maxpool_2])\n",
    "#time_distributed = TimeDistributed(concat_tensors)\n",
    "\n",
    "# flatten concatenated tensors\n",
    "\n",
    "flattened_tensor=TimeDistributed(Flatten())(concat_tensors)\n",
    "#flattened_tensor = Flatten()(concat_tensors)\n",
    "\n",
    "# dense layer (dense_1)\n",
    "dense_1 = Dense(HIDDEN_SIZE)(flattened_tensor)\n",
    "print(Dense_1.shape)\n",
    "\n",
    "\n",
    "# dropout_1\n",
    "dropout_1 = Dropout(drop)(dense_1)\n",
    "print(dropout_1.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nq9RWqNaU4C"
   },
   "source": [
    "If you want CNN layers to interact with the LSTM layer, they need to be distributed across time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDuERMw7-rAV"
   },
   "source": [
    "## BLSTM\n",
    "\n",
    "This is used to create LSTM layers. The data weâ€™re working with has temporal properties which we want to model as well â€” hence the use of a LSTM. You should create a BiLSTM. Try the output of cnn as the input for blstm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "id": "pFGp2EWI-fM7"
   },
   "outputs": [],
   "source": [
    "# BLSTM model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input,Dropout, InputLayer, Bidirectional, TimeDistributed, Activation, Embedding\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "#dropout_1 = Flatten()(dropout_1)\n",
    "#Input_2 = Input(shape=(MAX_LENGTH,),dtype='int32')\n",
    "#Embedding = Embedding(VOCAB_SIZE+1, EMBED_SIZE, input_length=MAX_LENGTH, mask_zero=True)(dropout_1)\n",
    "# Bidirectional 1\n",
    "Bidirectional_1 = Bidirectional(LSTM(HIDDEN_SIZE,return_sequences = True))(dropout_1)\n",
    "# Bidirectional 2\n",
    "Bidirectional_2 = Bidirectional(LSTM(HIDDEN_SIZE,return_sequences = False))(Bidirectional_1)\n",
    "# Dense layer (dense_2)\n",
    "dense_2 = Dense(HIDDEN_SIZE)(Bidirectional_2)\n",
    "# dropout_2\n",
    "dropout_2 = Dropout(drop)(dense_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wluAkx6AQUb"
   },
   "source": [
    "Concatenate 2 last layers and create the output layer. You need to concatenate the outputs of CNN and LSTM (dropout_1 and dropout_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {
    "id": "kzrhgkX2-fNE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 43)\n"
     ]
    }
   ],
   "source": [
    "# concatenate 2 final layers\n",
    "dropout_1flat = Flatten()(dropout_1)\n",
    "\n",
    "final_concat_output = Concatenate()([dropout_1flat,dropout_2])\n",
    "model_output = Dense(HIDDEN_SIZE)(final_concat_output)\n",
    "Activation = Activation(\"softmax\")(model_output)\n",
    "print(model_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_24\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_61 (InputLayer)           [(None, 137)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_65 (Embedding)        (None, 137, 100)     4373200     input_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "reshape_45 (Reshape)            (None, 137, 100, 1)  0           embedding_65[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 135, 1, 64)   19264       reshape_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 134, 1, 64)   25664       reshape_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 133, 1, 64)   32064       reshape_45[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 135, 1, 64)   256         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 134, 1, 64)   256         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 133, 1, 64)   256         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_114 (MaxPooling2D (None, 1, 1, 64)     0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_115 (MaxPooling2D (None, 1, 1, 64)     0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_116 (MaxPooling2D (None, 1, 1, 64)     0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_67 (Concatenate)    (None, 1, 1, 192)    0           max_pooling2d_114[0][0]          \n",
      "                                                                 max_pooling2d_115[0][0]          \n",
      "                                                                 max_pooling2d_116[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_22 (TimeDistri (None, 1, 192)       0           concatenate_67[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_48 (Dense)                (None, 1, 43)        8299        time_distributed_22[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_33 (Dropout)            (None, 1, 43)        0           dense_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_48 (Bidirectional (None, 1, 86)        29928       dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_49 (Bidirectional (None, 86)           44720       bidirectional_48[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_49 (Dense)                (None, 43)           3741        bidirectional_49[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_74 (Flatten)            (None, 43)           0           dropout_33[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_34 (Dropout)            (None, 43)           0           dense_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_68 (Concatenate)    (None, 86)           0           flatten_74[0][0]                 \n",
      "                                                                 dropout_34[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_50 (Dense)                (None, 43)           3741        concatenate_68[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 43)           0           dense_50[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,541,389\n",
      "Trainable params: 4,541,005\n",
      "Non-trainable params: 384\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# output\n",
    "model3 = Model(inputs=[Input_1], outputs=[Activation])\n",
    "model3.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "id": "3Jneg-GD-fNJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "274/274 [==============================] - 158s 555ms/step - loss: 0.1411 - accuracy: 0.4169 - val_loss: 0.1324 - val_accuracy: 0.6473\n",
      "Epoch 2/3\n",
      "274/274 [==============================] - 140s 512ms/step - loss: 0.0359 - accuracy: 0.7200 - val_loss: 0.0412 - val_accuracy: 0.6931\n",
      "Epoch 3/3\n",
      "274/274 [==============================] - 141s 514ms/step - loss: 0.0278 - accuracy: 0.7838 - val_loss: 0.0363 - val_accuracy: 0.7016\n"
     ]
    }
   ],
   "source": [
    "# Train the model - using validation \n",
    "\n",
    "Training = model3.fit(train_input,\n",
    "                    train_labels,\n",
    "                    epochs=3,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(val_input, val_labels),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "id": "MSMRSX1u-fNO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "560/560 [==============================] - 10s 18ms/step - loss: 0.0367 - accuracy: 0.7035\n"
     ]
    }
   ],
   "source": [
    "score = model3.evaluate(test_utterances_X, y_test, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "id": "3qFMsXNS-fNS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 70.35168409347534\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall Accuracy:\", score[1]*100)\n",
    "\n",
    "#Predicted labels\n",
    "label_pred = model3.predict(test_utterances_X, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bf accuracy: 16.577540106951872\n",
      "br accuracy: 2.3529411764705883\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix_balanced = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1))\n",
    "\n",
    "mat = plot_confusion_matrix(matrix_balanced)\n",
    "mat\n",
    "\n",
    "# Calculate Accuracies for \"br\" and \"bf\"\n",
    "\n",
    "mat = confusion_matrix(y_test.argmax(axis=1), label_pred.argmax(axis=1)) \n",
    "classes_accuracy = mat.diagonal()/mat.sum(axis=1)   #Calculates the accuracy of each individual classes\n",
    "\n",
    "\n",
    "bf_acc = classes_accuracy[15]   #bf class at index 15\n",
    "print(\"bf accuracy:\", bf_acc*100)\n",
    "\n",
    "br_acc = classes_accuracy[18]   #br class at index 18\n",
    "print(\"br accuracy:\", br_acc*100)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_10_Dialogue_Act_Tagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
