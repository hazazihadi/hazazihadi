{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pWgujXmGqzIC"
   },
   "source": [
    "# Sentence Classification with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pr5PWYKGPi6R"
   },
   "source": [
    "Turn a pre-trained BERT model into a trainable Keras layer and apply it to the semeval 2017. BERT (Bidirectional Embedding Representations from Transformers) is a new model for pre-training language representations that obtains state-of-the-art results on many NLP tasks. We demonstrate how to integrate BERT as a custom Keras layer to simplify model prototyping using huggingface. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2723,
     "status": "ok",
     "timestamp": 1618265007623,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "4spJ5PRhGJ1l"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Lambda, GlobalAveragePooling1D, Dense, Embedding\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import LSTM, RNN, Dropout, Input, LeakyReLU, Bidirectional,Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oimYsLssuSs"
   },
   "source": [
    "Before we start, we should install the huggingface transformer package. You can find the doc from its [website](https://huggingface.co/transformers/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9876,
     "status": "ok",
     "timestamp": 1618265042425,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "AElpzsKFiZSo",
    "outputId": "87e748f0-d91a-4c3d-81e5-462445dcae11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2MB 6.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3MB 23.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 39.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=f9f92b5b5471d971fdc9fd63fb330bb131e38acfa143c28126fcbdb304d2ce83\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: tokenizers, sacremoses, transformers\n",
      "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebbamBD0xu5z"
   },
   "source": [
    "## Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RbAMXQwqyVT8"
   },
   "source": [
    "In this lab we will use DistilBERT instead of BERT: DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, and runs 60% faster, while preserving over 95% of BERT’s performance as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "It is easy to switch between DistilBERT and BERT using the huggingface transformer package. This huggingface package provides many pre-trained and pre-built models that are easy to use via a few lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-cUTxt02dQb"
   },
   "source": [
    "Before using DistilBERT or BERT, we need a tokenizer. Generally speaking, every BERT related model has its own tokenizer, trained for that model (see this week's lecture video on sub-word tokenization). \n",
    "We can get the DistilBERT tokenizer from **DistilBertTokenizer.from_pretrained** function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164,
     "referenced_widgets": [
      "77087558efaa494d97cc18ce6098ff1d",
      "71de03e978254bcd8120a382162eca50",
      "4d4482c1e6d740e6b19490eb09b49195",
      "280050b334d54f0e82c549898f220080",
      "ba183926cdce4995850f751288a34b91",
      "8d6da20ee8264d62927f2e63d64e7ba1",
      "d79c592f69f841948ed60a6c0ac02854",
      "ffc23657088a40aaa51996a305591eb1",
      "a618a427313c4a9390201ca23ead44b9",
      "bd9087be463e46b49221aeef02ddd9d5",
      "5cf2e20802db4b119c379ba76a7b1dee",
      "6d7289573cbc4b3ca93183aadb894f68",
      "484e4710ae9d4ac1a7a159342e18ecc2",
      "bdb8372b854a4e75b7afe8c395b85e3e",
      "ae84712eb57e42869fcb01c5e0df5c35",
      "dad2fb72d3484eceb8117d230c1e7547",
      "77c070143b2c4ae48128fdc2a6b66e4e",
      "b10b8111904a4f2aa5bb64691ae01d90",
      "7e0bc5c07d0c4899ad7d080987f6fab8",
      "22fbdc8fa3a64c7ab035f44a078e08de",
      "55d378d1b10f49439b3a01b845cb8fa6",
      "13726a1c2b404246a0e5dd905daecb32",
      "37fabee0187947c4a22390cd3ad9f83e",
      "0970c5af03844623a991977f79d5bda9"
     ]
    },
    "executionInfo": {
     "elapsed": 2352,
     "status": "ok",
     "timestamp": 1618265044789,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "hKj6Y_TydjeS",
    "outputId": "7fcfe7d8-f800-4f09-ea6d-882c4f46288a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77087558efaa494d97cc18ce6098ff1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a618a427313c4a9390201ca23ead44b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77c070143b2c4ae48128fdc2a6b66e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, RobertaTokenizer \n",
    "import tqdm\n",
    "distil_bert = 'distilbert-base-uncased' # Pick any desired pre-trained model\n",
    "\n",
    "# Defining DistilBERT tokonizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(distil_bert, do_lower_case=True, add_special_tokens=True,\n",
    "                                                max_length=128, pad_to_max_length=True)\n",
    "\n",
    "def tokenize(sentences, tokenizer, pad_length=128, pad_to_max_length=True ):\n",
    "    if type(sentences) == str:\n",
    "        inputs = tokenizer.encode_plus(sentences, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        return np.asarray(inputs['input_ids'], dtype='int32'), np.asarray(inputs['attention_mask'], dtype='int32'), np.asarray(inputs['token_type_ids'], dtype='int32')\n",
    "    input_ids, input_masks, input_segments = [],[],[]\n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=pad_length, pad_to_max_length=pad_to_max_length, \n",
    "                                             return_attention_mask=True, return_token_type_ids=True)\n",
    "        input_ids.append(inputs['input_ids'])\n",
    "        input_masks.append(inputs['attention_mask'])\n",
    "        input_segments.append(inputs['token_type_ids'])        \n",
    "        \n",
    "    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32'), np.asarray(input_segments, dtype='int32')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sqMo6CPS3qJZ"
   },
   "source": [
    "Then we can use the tokenizer to tokenize the sentence. When working with word2vec and GloVe, we tokenized the sentence into words ourselves and then converted the tokens to GloVe word indices. But in BERT, we must use the BERT tokenizer: the tokens for BERT are different, and include whole words and sub-word tokens (see lecture video on sub-word tokenisation).\n",
    "\n",
    "For example, for the sentence: **This is a pretrained model.** our previous word-based tokenizer will generate the following tokens:\n",
    "\n",
    "**\"this\", \"is\", \"a\", \"pretrained\", \"model\", \".\"**\n",
    "\n",
    "Then you will find out that the word token \"pretrained\" is not in the GloVe word dictionary. Thus we can not assign a proper word vector for \"pretrained\".\n",
    "\n",
    "In BERT, the BERT tokenizer will separate the word \"pretrained\" into three sub-word tokens:\n",
    "\n",
    "**'pre', '##train', '##ed'**\n",
    "\n",
    "This way, BERT can use these three token vectors to represent the word \"pretrained\". Without the BERT tokenizer, it is hard to separate these unknown words properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1618265049223,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "iRoKe2DKyi41",
    "outputId": "8f98e025-dec6-46f8-fd0f-0053787dee06"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'capital', 'of', 'france', 'is', '[MASK]', '.'] \n",
      "\n",
      "['this', 'is', 'a', 'pre', '##train', '##ed', 'model', '.'] \n",
      "\n",
      "[ 101 1996 3007 1997 2605 2003  103 1012  102    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
      "\n",
      "[ 101 1996 3007 1997 2605 2003  103 1012  102]\n",
      "[1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.tokenize(\"The capital of France is [MASK].\")\n",
    "print(inputs,'\\n')\n",
    "\n",
    "inputs = tokenizer.tokenize(\"This is a pretrained model.\")\n",
    "print(inputs,'\\n')\n",
    "\n",
    "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer)\n",
    "print(ids)\n",
    "print(masks,\"\\n\")\n",
    "\n",
    "ids,masks,segments = tokenize(\"The capital of France is [MASK].\", tokenizer, pad_to_max_length=False)\n",
    "print(ids)\n",
    "print(masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J6OuZAA8sbdg"
   },
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqvPQvgvPv1W"
   },
   "source": [
    "### Downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1846,
     "status": "ok",
     "timestamp": 1618265055221,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "NyuSzkafqNca",
    "outputId": "0fea8eda-5545-4fb3-aa50-ab7c8c415a15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training entries: 17639\n",
      "Development entries: 1325\n",
      "Testing entries: 6185\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "def downloadfile(url):\n",
    "  rq = requests.get(url)\n",
    "  open(url.split('/')[-1], 'wb').write(rq.content)\n",
    "\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016devtest-BD.tsv')\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016dev-BD.tsv')\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016test-BD.tsv')\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2016train-BD.tsv')\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2015test-BD.tsv')\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/downloaded/twitter-2015train-BD.tsv')\n",
    "\n",
    "downloadfile('https://raw.githubusercontent.com/cbaziotis/datastories-semeval2017-task4/master/dataset/Subtask_BD/gold/SemEval2017-task4-test.subtask-BD.english.txt')\n",
    "\n",
    "with open('twitter-2016dev-BD.tsv', 'r',encoding='utf-8') as f:\n",
    "  dev_original = [l.strip().split('\\t') for l in f.readlines()]\n",
    "with open('SemEval2017-task4-test.subtask-BD.english.txt', 'r',encoding='utf-8') as f:\n",
    "  test_original = [l.strip().split('\\t') for l in f.readlines()]\n",
    "train_original = []\n",
    "with open('twitter-2016train-BD.tsv', 'r',encoding='utf-8') as f:\n",
    "  train_original = [l.strip().split('\\t') for l in f.readlines()]\n",
    "with open('twitter-2016test-BD.tsv', 'r',encoding='utf-8') as f:\n",
    "  train_original.extend([l.strip().split('\\t') for l in f.readlines()])\n",
    "with open('twitter-2016devtest-BD.tsv', 'r',encoding='utf-8') as f:\n",
    "  train_original.extend([l.strip().split('\\t') for l in f.readlines()])\n",
    "with open('twitter-2015test-BD.tsv', 'r',encoding='utf-8') as f:\n",
    "  train_original.extend([l.strip().split('\\t') for l in f.readlines() if l.strip().split('\\t')[2] in ['negative','positive']])\n",
    "with open('twitter-2015train-BD.tsv', 'r',encoding='utf-8') as f:\n",
    "  train_original.extend([l.strip().split('\\t') for l in f.readlines() if l.strip().split('\\t')[2] in ['negative','positive']])\n",
    "\n",
    "print(\"Training entries: {}\".format(len(train_original)))\n",
    "print(\"Development entries: {}\".format(len(dev_original)))\n",
    "print(\"Testing entries: {}\".format(len(test_original)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1618265058831,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "h-gjWRAuqg5s",
    "outputId": "8c57b313-4e33-416e-a9dc-36dafa4d53fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID \t TOPIC \t LABLE \t TWEET_TEXT\n",
      "['628949369883000832', '@microsoft', 'negative', \"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.\"]\n",
      "['628976607420645377', '@microsoft', 'negative', \"@Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it!\"]\n",
      "['629023169169518592', '@microsoft', 'negative', \"I may be ignorant on this issue but... should we celebrate @Microsoft's parental leave changes? Doesn't the gender divide suggest... (1/2)\"]\n",
      "['629179223232479232', '@microsoft', 'negative', 'Thanks to @microsoft, I just may be switching over to @apple.']\n",
      "['629226490152914944', '@microsoft', 'positive', 'Microsoft, I may not prefer your gaming branch of business. But, you do make a damn fine operating system. #Windows10 @Microsoft']\n"
     ]
    }
   ],
   "source": [
    "print(\"ID \\t TOPIC \\t LABLE \\t TWEET_TEXT\")\n",
    "print(train_original[0])\n",
    "print(train_original[1])\n",
    "print(train_original[2])\n",
    "print(train_original[3])\n",
    "print(train_original[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26426,
     "status": "ok",
     "timestamp": 1618265087976,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "gMCH1OoDrSNR",
    "outputId": "224e3a34-26f3-4108-d9c0-bfcaecd9e552"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dev_topic_int[0]:\n",
      "[ 101 2745 4027  102    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "x_dev_topic_masks[0]:\n",
      "[1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "x_dev_tweet_int[0]:\n",
      "[  101  6108  1062  9794 16021 23091  2007 16839  9080 12863  7050  2000\n",
      "  2745  4027  1024  6108  1062  4593  2587 16021 23091  2006  5095  1998\n",
      "  1012  1012  8299  1024  1013  1013  1056  1012  2522  1013  1053  3501\n",
      "  2683  2072  2549  8586  2615 18037   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "x_dev_tweet_masks[0]:\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to generate the following data\n",
    "#x_train_tweet_int\n",
    "#x_train_tweet_masks\n",
    "# x_train_topic_int\n",
    "# x_train_topic_masks\n",
    "\n",
    "# x_dev_tweet_int\n",
    "# x_dev_tweet_masks\n",
    "# x_dev_topic_int\n",
    "# x_dev_topic_masks\n",
    "\n",
    "# x_test_tweet_int\n",
    "# x_test_tweet_masks\n",
    "# x_test_topic_int\n",
    "# x_test_topic_masks\n",
    "\n",
    "x_train_tweetb4tokenize = []\n",
    "x_train_topicb4tokenize = []\n",
    "x_dev_tweetb4tokenize = []\n",
    "x_dev_topicb4tokenize = []\n",
    "x_test_tweetb4tokenize = []\n",
    "x_test_topicb4tokenize = []\n",
    "\n",
    "\n",
    "# your code goes here\n",
    "\n",
    "for w in train_original:                        #Extracts the tweet and topics from train set\n",
    "    x_train_tweetb4tokenize.append(w[3])\n",
    "    x_train_topicb4tokenize.append(w[1])\n",
    "\n",
    "for w in dev_original:                          #Extracts the tweet and topics from dev set\n",
    "    x_dev_tweetb4tokenize.append(w[3])\n",
    "    x_dev_topicb4tokenize.append(w[1])\n",
    "\n",
    "for w in test_original:                         #Extracts the tweet and topics from test set\n",
    "    x_test_tweetb4tokenize.append(w[3])\n",
    "    x_test_topicb4tokenize.append(w[1])\n",
    "\n",
    "x_train_tweet_int,x_train_tweet_masks,x_train_tweet_segments = tokenize(x_train_tweetb4tokenize, tokenizer)  \n",
    "x_train_topic_int,x_train_topic_masks,x_train_topic_segments = tokenize(x_train_topicb4tokenize, tokenizer)\n",
    "\n",
    "x_dev_tweet_int,x_dev_tweet_masks,x_dev_tweet_segments = tokenize(x_dev_tweetb4tokenize, tokenizer)\n",
    "x_dev_topic_int,x_dev_topic_masks,x_dev_topic_segments = tokenize(x_dev_topicb4tokenize, tokenizer)\n",
    "\n",
    "x_test_tweet_int,x_test_tweet_masks,x_test_tweet_segments = tokenize(x_test_tweetb4tokenize, tokenizer)  \n",
    "x_test_topic_int,x_test_topic_masks,x_test_topic_segments = tokenize(x_test_topicb4tokenize, tokenizer)\n",
    "\n",
    "# If use the previous tokenize function, you can get a print result like:\n",
    "assert len(x_train_topic_int) == len(train_original)\n",
    "assert len(x_train_topic_masks) == len(x_train_topic_int)\n",
    "assert len(x_test_topic_int) == len(test_original)\n",
    "assert len(x_test_topic_masks) == len(x_test_topic_int)\n",
    "print(\"x_dev_topic_int[0]:\")\n",
    "print(x_dev_topic_int[0])\n",
    "print(\"x_dev_topic_masks[0]:\")\n",
    "print(x_dev_topic_masks[0])\n",
    "print(\"x_dev_tweet_int[0]:\")\n",
    "print(x_dev_tweet_int[0])\n",
    "print(\"x_dev_tweet_masks[0]:\")\n",
    "print(x_dev_tweet_masks[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IreFXgruZot"
   },
   "source": [
    "We use 1 to represent \"positive\" and 0 for \"negative\" and generate the \"y\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1618265096850,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "abIb7Fe5u3GQ",
    "outputId": "1e447abe-d6cd-417a-f8c3-005cd87291d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def label2int(dataset):\n",
    "  y = []\n",
    "  for example in dataset:\n",
    "    if example[2].lower() == \"negative\":\n",
    "      y.append(0)\n",
    "    else:\n",
    "      y.append(1)\n",
    "  return y\n",
    "  \n",
    "y_train = label2int(train_original)\n",
    "y_dev = label2int(dev_original)\n",
    "y_test = label2int(test_original)\n",
    "y_train = np.array(y_train)\n",
    "y_dev = np.array(y_dev)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train[1])\n",
    "print(y_train[2])\n",
    "print(y_train[3])\n",
    "print(y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txxIQPEdFSgY"
   },
   "source": [
    "We also generate these target labels using one hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1618265099115,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "ohDm-E7k2w6c",
    "outputId": "b3c5cc5b-9a1a-4fe8-ae7b-b3d368749e06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[1 0]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "def int2onehot(dataset):\n",
    "  y = []\n",
    "  for example in dataset:\n",
    "    if example:\n",
    "      y.append(np.array([0,1]))\n",
    "    else:\n",
    "      y.append(np.array([1,0]))\n",
    "  return np.array(y)\n",
    "y_train_onehot = int2onehot(y_train)\n",
    "y_dev_onehot = int2onehot(y_dev)\n",
    "y_test_onehot = int2onehot(y_test)\n",
    "\n",
    "print(y_train_onehot[0])\n",
    "print(y_train_onehot[1])\n",
    "print(y_train_onehot[2])\n",
    "print(y_train_onehot[3])\n",
    "print(y_train_onehot[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 289,
     "status": "ok",
     "timestamp": 1618265102140,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "6Kos8Yp6HDZ3",
    "outputId": "bfedba19-6f97-44e1-a65a-94bf5e3c488f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  101  1026 19802  1028   102     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "start = [\"<SEP>\"]\n",
    "sep ,masks,segments = tokenize(start, tokenizer)\n",
    "print(sep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TnnSuspvC5b"
   },
   "source": [
    "he easiest way is to input the tweet and topic as paired sentences into the model, concatenating them and separating them by the special BERT sentence-separator token [SEP] Then we can apply BERT directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21897,
     "status": "ok",
     "timestamp": 1618265126244,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "nKOiVVXQu-_I",
    "outputId": "28bfb3ca-5e70-475d-d068-cb63d6136770"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2079: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  101  6108  1062  9794 16021 23091  2007 16839  9080 12863  7050  2000\n",
      "  2745  4027  1024  6108  1062  4593  2587 16021 23091  2006  5095  1998\n",
      "  1012  1012  8299  1024  1013  1013  1056  1012  2522  1013  1053  3501\n",
      "  2683  2072  2549  8586  2615 18037   102  2745  4027   102     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] \n",
      "\n",
      "[  101  6108  1062  9794 16021 23091  2007 16839  9080 12863  7050  2000\n",
      "  2745  4027  1024  6108  1062  4593  2587 16021 23091  2006  5095  1998\n",
      "  1012  1012  8299  1024  1013  1013  1056  1012  2522  1013  1053  3501\n",
      "  2683  2072  2549  8586  2615 18037   102  2745  4027   102     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[\"dear @Microsoft the newOoffice for Mac is great and all, but no Lync update? C'mon.[SEP]@microsoft\", \"@Microsoft how about you make a system that doesn't eat my friggin discs. This is the 2nd time this has happened and I am so sick of it![SEP]@microsoft\"]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code to combine the tweet and topic into the following varibles\n",
    "\n",
    "x_train_combined_b4_tokenize = []\n",
    "x_dev_combined_b4_tokenize = []\n",
    "x_test_combined_b4_tokenize = []\n",
    "\n",
    "\n",
    "\n",
    "sep = \"[SEP]\"\n",
    "#sep ,masks,segments = tokenize(start, tokenizer)\n",
    "\n",
    "# Tips: \n",
    "# 1) We can use the special token <SEP> to concatenate the tweets and topics.\n",
    "# 2) After combine them, make sure they are paded.\n",
    "\n",
    "# your code goes here\n",
    "for w in range (len(x_dev_tweetb4tokenize)):                 # Combining the tweets and the topics in the train set\n",
    "    x_dev_combined = x_dev_tweetb4tokenize[w] + sep + x_dev_topicb4tokenize[w]   # Adds [SEP] in between\n",
    "    x_dev_combined_b4_tokenize.append(x_dev_combined)\n",
    "\n",
    "\n",
    "for w in range (len(x_train_tweetb4tokenize)):                 # Combining the tweets and the topics in the train set\n",
    "    x_train_combined = x_train_tweetb4tokenize[w] + sep + x_train_topicb4tokenize[w]   # Adds [SEP] in between\n",
    "    x_train_combined_b4_tokenize.append(x_train_combined)\n",
    "\n",
    "for w in range (len(x_test_tweetb4tokenize)):                 # Combining the tweets and the topics in the train set\n",
    "    x_test_combined = x_test_tweetb4tokenize[w] + sep + x_test_topicb4tokenize[w]   # Adds [SEP] in between\n",
    "    x_test_combined_b4_tokenize.append(x_test_combined)\n",
    "\n",
    "x_train_int, x_train_masks, x_train_segments = tokenize(x_train_combined_b4_tokenize, tokenizer)  #tokenize combined sentences\n",
    "x_dev_int, x_dev_masks, x_dev_segments = tokenize(x_dev_combined_b4_tokenize, tokenizer) \n",
    "x_test_int, x_test_masks, x_test_segments = tokenize(x_test_combined_b4_tokenize, tokenizer) \n",
    "\n",
    "\n",
    "# Don't forget the to use np.array function to wrap the output of pad_sequences function\n",
    "x_train_int_np = np.array(x_train_int)\n",
    "x_train_masks_np = np.array(x_train_masks)\n",
    "x_dev_int_np = np.array(x_dev_int)\n",
    "x_dev_masks_np = np.array(x_dev_masks)\n",
    "x_test_int_np = np.array(x_test_int)\n",
    "x_test_masks_np = np.array(x_test_masks)\n",
    "\n",
    "\n",
    "print(x_dev_int[0])\n",
    "print(x_dev_masks[0],'\\n')\n",
    "print(x_dev_int_np[0])\n",
    "print(x_dev_masks_np[0])\n",
    "\n",
    "print(x_train_combined_b4_tokenize[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFlpl780HDZ4"
   },
   "source": [
    "In your report, show a few examples of your input, with text and topic included but separated by [SEP]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqvUGIwwGJqu"
   },
   "source": [
    "## Model 1: Prebuilt Sequence Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSxC41ln07im"
   },
   "source": [
    "The huggingface transformer package provides many prebuilt models. First, let us try a basic text classification model based on DistilBERT. This first method is the standard way BERT models are used for text classification: we use the embedding of the special [CLS] token at the beginning of the sequence, and put it through a simple classifier layer to make the decision. The TFDistilBertForSequenceClassification class takes care of that for us.\n",
    "\n",
    "The models with BERT are much bigger than our previous models. To run it faster, we can use TPU here. The detailed guideline about using TPU can be found from https://www.tensorflow.org/guide/tpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "85c0434ce715483ab3752a908abc7721",
      "c9056b68f41641afa9d3361dfa50aad6",
      "2f04848439124fd1a8441890c16d384a",
      "42e4eb3618ca42dc9517c402a9f12bd8",
      "37d232df20334ac6b722a7d76ec399b1",
      "ab4ba080f4db4dd69b5c18fe10572da5",
      "0947bef362944d77a3d11667b5331a6b",
      "2d681fac3e8f459b9d544a555f204401"
     ]
    },
    "executionInfo": {
     "elapsed": 93392,
     "status": "ok",
     "timestamp": 1618265231067,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "1gXFbb2cxBlw",
    "outputId": "5cb4fe77-6d60-4f4f-8374-09c39dfb589f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Entering into master device scope: /job:worker/replica:0/task:0/device:CPU:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.78.16.18:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.78.16.18:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85c0434ce715483ab3752a908abc7721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=363423424.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f99960a6d70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f99960a6d70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f99960a6d70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f99b194fc20> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function wrap at 0x7f99b194fc20> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING: AutoGraph could not transform <function wrap at 0x7f99b194fc20> and will run it as-is.\n",
      "Cause: while/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification, DistilBertConfig\n",
    "import tensorflow as tf\n",
    "\n",
    "distil_bert = 'distilbert-base-uncased'\n",
    "\n",
    "config = DistilBertConfig(num_labels=2)\n",
    "config.output_hidden_states = False\n",
    "\n",
    "def create_TFDistilBertForSequenceClassification():\n",
    "    transformer_model = TFDistilBertForSequenceClassification.from_pretrained(distil_bert, config = config)\n",
    "    input_ids = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "    input_masks_ids = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32')\n",
    "    X = transformer_model(input_ids, input_masks_ids)\n",
    "    return tf.keras.Model(inputs=[input_ids, input_masks_ids], outputs = X)\n",
    "\n",
    "use_tpu = True\n",
    "if use_tpu:\n",
    "  # Create distribution strategy\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "  # Create model on TPU:\n",
    "    with strategy.scope():\n",
    "        model = create_TFDistilBertForSequenceClassification()\n",
    "        optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "    model = create_TFDistilBertForSequenceClassification()\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 61387,
     "status": "ok",
     "timestamp": 1618265231068,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "2CPFj0CMx9mw",
    "outputId": "2aade98d-37ec-4aad-c038-09283ce68b7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_for_sequence_cla TFSequenceClassifier 66955010    input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355654,
     "status": "ok",
     "timestamp": 1618265531966,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "zQQH5lE_33Vn",
    "outputId": "be924bd9-097e-4e45-b2a4-137133fe4c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 0s - loss: 0.6505 - accuracy: 0.7890WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "35/35 [==============================] - 90s 1s/step - loss: 0.6462 - accuracy: 0.7898 - val_loss: 0.4516 - val_accuracy: 0.7683\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.3472 - accuracy: 0.8662 - val_loss: 0.3522 - val_accuracy: 0.8438\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.2500 - accuracy: 0.9137 - val_loss: 0.4642 - val_accuracy: 0.8423\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.2398 - accuracy: 0.9182 - val_loss: 0.4234 - val_accuracy: 0.8513\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 7s 208ms/step - loss: 0.1800 - accuracy: 0.9431 - val_loss: 0.4448 - val_accuracy: 0.8317\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.2212 - accuracy: 0.9216 - val_loss: 0.6452 - val_accuracy: 0.8453\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.1330 - accuracy: 0.9626 - val_loss: 0.7597 - val_accuracy: 0.8506\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.1012 - accuracy: 0.9751 - val_loss: 0.7730 - val_accuracy: 0.8151\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.1275 - accuracy: 0.9612 - val_loss: 1.0022 - val_accuracy: 0.8453\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0999 - accuracy: 0.9746 - val_loss: 0.9818 - val_accuracy: 0.8332\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0928 - accuracy: 0.9786 - val_loss: 1.3007 - val_accuracy: 0.8415\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0565 - accuracy: 0.9893 - val_loss: 1.5313 - val_accuracy: 0.8355\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0414 - accuracy: 0.9916 - val_loss: 1.4425 - val_accuracy: 0.8415\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0324 - accuracy: 0.9950 - val_loss: 1.5982 - val_accuracy: 0.8385\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0277 - accuracy: 0.9966 - val_loss: 1.7157 - val_accuracy: 0.8445\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0258 - accuracy: 0.9968 - val_loss: 1.6607 - val_accuracy: 0.8430\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0451 - accuracy: 0.9939 - val_loss: 1.8532 - val_accuracy: 0.8234\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0835 - accuracy: 0.9845 - val_loss: 1.3220 - val_accuracy: 0.8438\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 7s 208ms/step - loss: 0.0422 - accuracy: 0.9943 - val_loss: 1.5546 - val_accuracy: 0.8362\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0328 - accuracy: 0.9951 - val_loss: 1.5931 - val_accuracy: 0.8445\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0254 - accuracy: 0.9977 - val_loss: 1.7736 - val_accuracy: 0.8430\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 8s 224ms/step - loss: 0.0417 - accuracy: 0.9960 - val_loss: 1.5557 - val_accuracy: 0.8483\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0268 - accuracy: 0.9980 - val_loss: 1.7301 - val_accuracy: 0.8513\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0210 - accuracy: 0.9983 - val_loss: 1.6453 - val_accuracy: 0.8423\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0270 - accuracy: 0.9973 - val_loss: 1.8071 - val_accuracy: 0.8475\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0303 - accuracy: 0.9976 - val_loss: 1.8767 - val_accuracy: 0.8385\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0302 - accuracy: 0.9976 - val_loss: 1.7990 - val_accuracy: 0.8491\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0203 - accuracy: 0.9986 - val_loss: 1.8190 - val_accuracy: 0.8408\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0155 - accuracy: 0.9986 - val_loss: 1.8158 - val_accuracy: 0.8460\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0261 - accuracy: 0.9981 - val_loss: 1.8675 - val_accuracy: 0.8453\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([x_train_int_np,x_train_masks_np],\n",
    "                    y_train_onehot,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev_onehot),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8860,
     "status": "ok",
     "timestamp": 1618267067750,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "lQcytuBdaWVp",
    "outputId": "8e9b9b9d-5b0e-41ae-b28e-eb4340ee0d2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 8s 31ms/step - loss: 1.4850 - accuracy: 0.8627\n",
      "[1.484992504119873, 0.8627324104309082]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate([x_test_int_np,x_test_masks_np], y_test_onehot)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vdZ4nl08vp9A"
   },
   "source": [
    "\n",
    "## Model 2: Neural bag of words using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gyCwXFj_R5w"
   },
   "source": [
    "In this model, we take the NBOW classifier from lab 4 (model3-1) and integrate BERT. Instead of averaging over word2vec or GloVe word vectors, we are averaging over the embedding representations produced by BERT - but otherwise, the classifier is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1618267217526,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "DStlnRQRf-4v"
   },
   "outputs": [],
   "source": [
    "class GlobalAveragePooling1DMasked(GlobalAveragePooling1D):\n",
    "    def call(self, x, mask=None):\n",
    "        if mask != None:\n",
    "            return K.sum(x, axis=1) / K.sum(mask, axis=1)\n",
    "        else:\n",
    "            return super().call(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1618267218731,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "8fTwmYDvNEyT"
   },
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertModel, DistilBertConfig\n",
    "\n",
    "def get_BERT_layer():\n",
    "  distil_bert = 'distilbert-base-uncased'\n",
    "  config = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\n",
    "  config.output_hidden_states = False\n",
    "  return TFDistilBertModel.from_pretrained(distil_bert, config = config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31847,
     "status": "ok",
     "timestamp": 1618267251523,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "VICS9rY8C7KH",
    "outputId": "ee532d66-5aa2-4f78-8409-ffb8672cee8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.78.16.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.78.16.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.78.16.18:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.78.16.18:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Model2_BERT\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model (TFDistilB TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_masked (None, 768)          0           tf_distil_bert_model[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           12304       global_average_pooling1d_masked[0\n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            17          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 66,375,201\n",
      "Trainable params: 66,375,201\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hdepth=16\n",
    "MAX_SEQUENCE_LENGTH = 128\n",
    "EMBED_SIZE=100\n",
    "\n",
    "\n",
    "def create_bag_of_words_BERT():\n",
    "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "  bert_embeddings = get_BERT_layer()\n",
    "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "\n",
    "  pooled_sent=GlobalAveragePooling1DMasked()(embedded_sent)\n",
    "  hidden_output=Dense(hdepth,input_shape=(MAX_SEQUENCE_LENGTH,EMBED_SIZE),activation='sigmoid',kernel_initializer='glorot_uniform')(pooled_sent) # Sigmoid\n",
    "  label=Dense(1,input_shape=(hdepth,),activation='sigmoid',kernel_initializer='glorot_uniform')(hidden_output)\n",
    "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[label],name='Model2_BERT')\n",
    "\n",
    "use_tpu = True\n",
    "if use_tpu:\n",
    "  # Create distribution strategy\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "  # Create model\n",
    "  with strategy.scope():\n",
    "    model2 = create_bag_of_words_BERT()\n",
    "    optimizer2 = keras.optimizers.Adam(lr=5e-5)\n",
    "    model2.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "else:\n",
    "  model2 = create_bag_of_words_BERT()\n",
    "  model2.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.summary() \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331748,
     "status": "ok",
     "timestamp": 1618267554324,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "S0SbsCsxF1zi",
    "outputId": "c3950905-4893-4555-9c99-c93015812d23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 0s - loss: 0.4270 - accuracy: 0.8171WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "35/35 [==============================] - 93s 1s/step - loss: 0.4253 - accuracy: 0.8179 - val_loss: 0.3329 - val_accuracy: 0.8468\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 8s 222ms/step - loss: 0.2449 - accuracy: 0.9045 - val_loss: 0.3313 - val_accuracy: 0.8430\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 7s 203ms/step - loss: 0.2036 - accuracy: 0.9285 - val_loss: 0.3418 - val_accuracy: 0.8506\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.1680 - accuracy: 0.9492 - val_loss: 0.3443 - val_accuracy: 0.8475\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.1527 - accuracy: 0.9575 - val_loss: 0.3617 - val_accuracy: 0.8325\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.1276 - accuracy: 0.9704 - val_loss: 0.3824 - val_accuracy: 0.8445\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.1152 - accuracy: 0.9764 - val_loss: 0.4008 - val_accuracy: 0.8234\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.1108 - accuracy: 0.9785 - val_loss: 0.3953 - val_accuracy: 0.8377\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.1034 - accuracy: 0.9819 - val_loss: 0.3897 - val_accuracy: 0.8355\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.1024 - accuracy: 0.9822 - val_loss: 0.4182 - val_accuracy: 0.8415\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0910 - accuracy: 0.9880 - val_loss: 0.4285 - val_accuracy: 0.8377\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0895 - accuracy: 0.9882 - val_loss: 0.4209 - val_accuracy: 0.8445\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 7s 203ms/step - loss: 0.0856 - accuracy: 0.9898 - val_loss: 0.4094 - val_accuracy: 0.8325\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0856 - accuracy: 0.9885 - val_loss: 0.4354 - val_accuracy: 0.8340\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0824 - accuracy: 0.9907 - val_loss: 0.4135 - val_accuracy: 0.8309\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0825 - accuracy: 0.9900 - val_loss: 0.4456 - val_accuracy: 0.8408\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0872 - accuracy: 0.9885 - val_loss: 0.4247 - val_accuracy: 0.8468\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0833 - accuracy: 0.9894 - val_loss: 0.4244 - val_accuracy: 0.8445\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0799 - accuracy: 0.9912 - val_loss: 0.4405 - val_accuracy: 0.8272\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0799 - accuracy: 0.9908 - val_loss: 0.4306 - val_accuracy: 0.8279\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0804 - accuracy: 0.9896 - val_loss: 0.4409 - val_accuracy: 0.8377\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0749 - accuracy: 0.9924 - val_loss: 0.4334 - val_accuracy: 0.8294\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0768 - accuracy: 0.9917 - val_loss: 0.4519 - val_accuracy: 0.8408\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0696 - accuracy: 0.9940 - val_loss: 0.4433 - val_accuracy: 0.8408\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0728 - accuracy: 0.9918 - val_loss: 0.4285 - val_accuracy: 0.8385\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0717 - accuracy: 0.9927 - val_loss: 0.4469 - val_accuracy: 0.8445\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 7s 205ms/step - loss: 0.0716 - accuracy: 0.9933 - val_loss: 0.4505 - val_accuracy: 0.8370\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 7s 204ms/step - loss: 0.0748 - accuracy: 0.9912 - val_loss: 0.4295 - val_accuracy: 0.8498\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 7s 207ms/step - loss: 0.0694 - accuracy: 0.9928 - val_loss: 0.4290 - val_accuracy: 0.8438\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 7s 206ms/step - loss: 0.0693 - accuracy: 0.9934 - val_loss: 0.4306 - val_accuracy: 0.8317\n"
     ]
    }
   ],
   "source": [
    "\n",
    "history = model2.fit([x_train_int_np,x_train_masks_np],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 339843,
     "status": "ok",
     "timestamp": 1618267562757,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "rs0_vvG6UQtv",
    "outputId": "5e24ae30-8eef-42ce-bc20-3e1d1b80d7e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 8s 30ms/step - loss: 0.4000 - accuracy: 0.8862\n",
      "[0.39996054768562317, 0.8861762285232544]\n"
     ]
    }
   ],
   "source": [
    "results = model2.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9jh0DLrSVr0W"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awOphcCnhEwv"
   },
   "source": [
    "## Model 3: CNN or LSTM with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dgern1vfVthf"
   },
   "outputs": [],
   "source": [
    "results = modelcnn.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35059,
     "status": "ok",
     "timestamp": 1618276343841,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "FA5rmoz0spdy",
    "outputId": "bd48f2ac-691b-451f-c0c8-8961f6238ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.78.16.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.78.16.18:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.78.16.18:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: grpc://10.78.16.18:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_token (InputLayer)        [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masked_token (InputLayer)       [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_distil_bert_model_9 (TFDisti TFBaseModelOutput(la 66362880    input_token[0][0]                \n",
      "                                                                 masked_token[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   (None, 100)          347600      tf_distil_bert_model_9[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 1)            101         lstm_5[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 66,710,581\n",
      "Trainable params: 66,710,581\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "\n",
    "EMBED_SIZE = 100\n",
    "\n",
    "#Diff style\n",
    "def create_LSTM():\n",
    "  input_ids_in = tf.keras.layers.Input(shape=(128,), name='input_token', dtype='int32')\n",
    "  input_masks_in = tf.keras.layers.Input(shape=(128,), name='masked_token', dtype='int32') \n",
    "\n",
    "  bert_embeddings = get_BERT_layer()\n",
    "  embedded_sent = bert_embeddings(input_ids_in, attention_mask=input_masks_in)[0]\n",
    "\n",
    "  lstm = LSTM(100, return_sequences=False)(embedded_sent)\n",
    "  DenseOutput = Dense(1, activation='softmax')(lstm)\n",
    "  return Model(inputs=[input_ids_in,input_masks_in], outputs=[DenseOutput])  \n",
    "\n",
    "use_tpu = True\n",
    "if use_tpu:\n",
    "  # Create distribution strategy\n",
    "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "  tf.config.experimental_connect_to_cluster(tpu)\n",
    "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "  strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "\n",
    "  # Create model\n",
    "  with strategy.scope():\n",
    "    modellstm = create_LSTM()\n",
    "    optimizer2 = keras.optimizers.Adam(lr=5e-5)\n",
    "    modellstm.compile(optimizer=optimizer2, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "else:\n",
    "  modellstm = create_LSTM()\n",
    "  modellstm.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "modellstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GN012X0zwhK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 317723,
     "status": "ok",
     "timestamp": 1618276697333,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "ChtI_XZZs686",
    "outputId": "75b1623f-acbb-4f3b-9d99-35228dcadaad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - ETA: 0s - loss: 0.4642 - accuracy: 0.7894WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r",
      "35/35 [==============================] - 97s 1s/step - loss: 0.4618 - accuracy: 0.7895 - val_loss: 0.3378 - val_accuracy: 0.7442\n",
      "Epoch 2/30\n",
      "35/35 [==============================] - 10s 280ms/step - loss: 0.2302 - accuracy: 0.7967 - val_loss: 0.3138 - val_accuracy: 0.7442\n",
      "Epoch 3/30\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.1646 - accuracy: 0.7876 - val_loss: 0.3186 - val_accuracy: 0.7442\n",
      "Epoch 4/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.1201 - accuracy: 0.7886 - val_loss: 0.3672 - val_accuracy: 0.7442\n",
      "Epoch 5/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0843 - accuracy: 0.7929 - val_loss: 0.3881 - val_accuracy: 0.7442\n",
      "Epoch 6/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0595 - accuracy: 0.7914 - val_loss: 0.4347 - val_accuracy: 0.7442\n",
      "Epoch 7/30\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.0412 - accuracy: 0.7893 - val_loss: 0.5607 - val_accuracy: 0.7442\n",
      "Epoch 8/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0336 - accuracy: 0.7880 - val_loss: 0.5970 - val_accuracy: 0.7442\n",
      "Epoch 9/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0231 - accuracy: 0.7931 - val_loss: 0.6010 - val_accuracy: 0.7442\n",
      "Epoch 10/30\n",
      "35/35 [==============================] - 7s 211ms/step - loss: 0.0160 - accuracy: 0.7949 - val_loss: 0.7126 - val_accuracy: 0.7442\n",
      "Epoch 11/30\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.0208 - accuracy: 0.7900 - val_loss: 0.6733 - val_accuracy: 0.7442\n",
      "Epoch 12/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0151 - accuracy: 0.7906 - val_loss: 0.6647 - val_accuracy: 0.7442\n",
      "Epoch 13/30\n",
      "35/35 [==============================] - 7s 215ms/step - loss: 0.0164 - accuracy: 0.7917 - val_loss: 0.7225 - val_accuracy: 0.7442\n",
      "Epoch 14/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0094 - accuracy: 0.7937 - val_loss: 0.7084 - val_accuracy: 0.7442\n",
      "Epoch 15/30\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.0111 - accuracy: 0.7918 - val_loss: 0.7237 - val_accuracy: 0.7442\n",
      "Epoch 16/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0080 - accuracy: 0.7918 - val_loss: 0.7521 - val_accuracy: 0.7442\n",
      "Epoch 17/30\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.0092 - accuracy: 0.7922 - val_loss: 0.7345 - val_accuracy: 0.7442\n",
      "Epoch 18/30\n",
      "35/35 [==============================] - 7s 212ms/step - loss: 0.0081 - accuracy: 0.7925 - val_loss: 0.7891 - val_accuracy: 0.7442\n",
      "Epoch 19/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0086 - accuracy: 0.7861 - val_loss: 0.7789 - val_accuracy: 0.7442\n",
      "Epoch 20/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0076 - accuracy: 0.7969 - val_loss: 0.8132 - val_accuracy: 0.7442\n",
      "Epoch 21/30\n",
      "35/35 [==============================] - 7s 215ms/step - loss: 0.0064 - accuracy: 0.7848 - val_loss: 0.7933 - val_accuracy: 0.7442\n",
      "Epoch 22/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0052 - accuracy: 0.7903 - val_loss: 0.7504 - val_accuracy: 0.7442\n",
      "Epoch 23/30\n",
      "35/35 [==============================] - 8s 217ms/step - loss: 0.0028 - accuracy: 0.7936 - val_loss: 0.8339 - val_accuracy: 0.7442\n",
      "Epoch 24/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0064 - accuracy: 0.7905 - val_loss: 0.7789 - val_accuracy: 0.7442\n",
      "Epoch 25/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0073 - accuracy: 0.7913 - val_loss: 0.8219 - val_accuracy: 0.7442\n",
      "Epoch 26/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0044 - accuracy: 0.7899 - val_loss: 0.8430 - val_accuracy: 0.7442\n",
      "Epoch 27/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0047 - accuracy: 0.7917 - val_loss: 0.7642 - val_accuracy: 0.7442\n",
      "Epoch 28/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0053 - accuracy: 0.7903 - val_loss: 0.8461 - val_accuracy: 0.7442\n",
      "Epoch 29/30\n",
      "35/35 [==============================] - 7s 213ms/step - loss: 0.0049 - accuracy: 0.7900 - val_loss: 0.8307 - val_accuracy: 0.7442\n",
      "Epoch 30/30\n",
      "35/35 [==============================] - 7s 214ms/step - loss: 0.0035 - accuracy: 0.7915 - val_loss: 0.8161 - val_accuracy: 0.7442\n"
     ]
    }
   ],
   "source": [
    "history = modellstm.fit([x_train_int_np,x_train_masks_np],\n",
    "                    y_train,\n",
    "                    epochs=30,\n",
    "                    batch_size=512,\n",
    "                    validation_data=([x_dev_int_np,x_dev_masks_np], y_dev),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5395,
     "status": "ok",
     "timestamp": 1618276781889,
     "user": {
      "displayName": "Hazazi Hadi",
      "photoUrl": "",
      "userId": "04178049625770165817"
     },
     "user_tz": -60
    },
    "id": "3CmrIpLczzpI",
    "outputId": "9639b660-9020-4b8d-969b-0564278be326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 5s 24ms/step - loss: 0.6879 - accuracy: 0.3982\n",
      "[0.6879350543022156, 0.398221492767334]\n"
     ]
    }
   ],
   "source": [
    "results = modellstm.evaluate([x_test_int_np,x_test_masks_np], y_test)\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_9_Sentence_Classification_with_BERT(Student).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0947bef362944d77a3d11667b5331a6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0970c5af03844623a991977f79d5bda9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "13726a1c2b404246a0e5dd905daecb32": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "22fbdc8fa3a64c7ab035f44a078e08de": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0970c5af03844623a991977f79d5bda9",
      "placeholder": "​",
      "style": "IPY_MODEL_37fabee0187947c4a22390cd3ad9f83e",
      "value": " 466k/466k [00:00&lt;00:00, 2.12MB/s]"
     }
    },
    "280050b334d54f0e82c549898f220080": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ffc23657088a40aaa51996a305591eb1",
      "placeholder": "​",
      "style": "IPY_MODEL_d79c592f69f841948ed60a6c0ac02854",
      "value": " 232k/232k [00:01&lt;00:00, 201kB/s]"
     }
    },
    "2d681fac3e8f459b9d544a555f204401": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f04848439124fd1a8441890c16d384a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ab4ba080f4db4dd69b5c18fe10572da5",
      "max": 363423424,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_37d232df20334ac6b722a7d76ec399b1",
      "value": 363423424
     }
    },
    "37d232df20334ac6b722a7d76ec399b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "37fabee0187947c4a22390cd3ad9f83e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "42e4eb3618ca42dc9517c402a9f12bd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d681fac3e8f459b9d544a555f204401",
      "placeholder": "​",
      "style": "IPY_MODEL_0947bef362944d77a3d11667b5331a6b",
      "value": " 363M/363M [00:11&lt;00:00, 33.0MB/s]"
     }
    },
    "484e4710ae9d4ac1a7a159342e18ecc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "4d4482c1e6d740e6b19490eb09b49195": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d6da20ee8264d62927f2e63d64e7ba1",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ba183926cdce4995850f751288a34b91",
      "value": 231508
     }
    },
    "55d378d1b10f49439b3a01b845cb8fa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "5cf2e20802db4b119c379ba76a7b1dee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bdb8372b854a4e75b7afe8c395b85e3e",
      "max": 28,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_484e4710ae9d4ac1a7a159342e18ecc2",
      "value": 28
     }
    },
    "6d7289573cbc4b3ca93183aadb894f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dad2fb72d3484eceb8117d230c1e7547",
      "placeholder": "​",
      "style": "IPY_MODEL_ae84712eb57e42869fcb01c5e0df5c35",
      "value": " 28.0/28.0 [00:00&lt;00:00, 51.9B/s]"
     }
    },
    "71de03e978254bcd8120a382162eca50": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77087558efaa494d97cc18ce6098ff1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d4482c1e6d740e6b19490eb09b49195",
       "IPY_MODEL_280050b334d54f0e82c549898f220080"
      ],
      "layout": "IPY_MODEL_71de03e978254bcd8120a382162eca50"
     }
    },
    "77c070143b2c4ae48128fdc2a6b66e4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7e0bc5c07d0c4899ad7d080987f6fab8",
       "IPY_MODEL_22fbdc8fa3a64c7ab035f44a078e08de"
      ],
      "layout": "IPY_MODEL_b10b8111904a4f2aa5bb64691ae01d90"
     }
    },
    "7e0bc5c07d0c4899ad7d080987f6fab8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Downloading: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13726a1c2b404246a0e5dd905daecb32",
      "max": 466062,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_55d378d1b10f49439b3a01b845cb8fa6",
      "value": 466062
     }
    },
    "85c0434ce715483ab3752a908abc7721": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f04848439124fd1a8441890c16d384a",
       "IPY_MODEL_42e4eb3618ca42dc9517c402a9f12bd8"
      ],
      "layout": "IPY_MODEL_c9056b68f41641afa9d3361dfa50aad6"
     }
    },
    "8d6da20ee8264d62927f2e63d64e7ba1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a618a427313c4a9390201ca23ead44b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5cf2e20802db4b119c379ba76a7b1dee",
       "IPY_MODEL_6d7289573cbc4b3ca93183aadb894f68"
      ],
      "layout": "IPY_MODEL_bd9087be463e46b49221aeef02ddd9d5"
     }
    },
    "ab4ba080f4db4dd69b5c18fe10572da5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae84712eb57e42869fcb01c5e0df5c35": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b10b8111904a4f2aa5bb64691ae01d90": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ba183926cdce4995850f751288a34b91": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "bd9087be463e46b49221aeef02ddd9d5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdb8372b854a4e75b7afe8c395b85e3e": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9056b68f41641afa9d3361dfa50aad6": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d79c592f69f841948ed60a6c0ac02854": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dad2fb72d3484eceb8117d230c1e7547": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffc23657088a40aaa51996a305591eb1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
