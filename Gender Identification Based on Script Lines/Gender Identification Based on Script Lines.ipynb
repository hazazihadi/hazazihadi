{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  The classification reports are shown at the bottom of the notebook ##########\n",
    "\n",
    "import csv                               # csv reader\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.classify import SklearnClassifier\n",
    "from random import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import array\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.tag import CRFTagger\n",
    "from collections import Counter\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tree import Tree\n",
    "from nltk.chunk import tree2conlltags\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacystring(Text):  #Cleans the data and output spacy string and is used in toFeatureVector to tag token using spacy function\n",
    "    \n",
    "    string.punctuation\n",
    "\n",
    "    # word tokenisation\n",
    "    Text = Text.strip(string.punctuation)\n",
    "    Text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", Text) #separating words and special charaters from words that comes after the word\n",
    "    Text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", Text) #separating words and special charaters from words that comes before the word\n",
    "    Text =  re.sub(r'[^\\w\\s]','',Text)\n",
    "    Text2 = re.sub(r'\\_','',Text)\n",
    "    \n",
    "\n",
    "    doc = nlp(Text2)                   #transforms sentence into spacy string\n",
    "    \n",
    "    return doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(path, Text=None):\n",
    "    with open(path, encoding = 'utf-8') as f:                                                                                           \n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "        for line in csv_reader:\n",
    "            (Lines, Char, Gender) = parseReview(line) #adds data as tuple to be appended to rawData\n",
    "            rawData.append((Lines, Char, Gender)) #appends 3 features\n",
    "            \n",
    "def loadTest(path, Text=None):\n",
    "    with open(path, encoding = 'utf-8') as f:                                                                                           \n",
    "        csv_reader = csv.reader(f, delimiter=',')\n",
    "\n",
    "        for line in csv_reader:\n",
    "            (Lines, Char, Gender) = parseReview(line) #adds data as tuple to be appended to rawData\n",
    "            rawTest.append((Lines, Char, Gender)) #appends 3 features\n",
    "\n",
    "            \n",
    "def splitData(percentage):\n",
    "    # A method to split the data between trainData and testData \n",
    "    dataSamples = len(rawData)\n",
    "    halfOfData = int(len(rawData)/2)\n",
    "    trainingSamples = int((percentage*dataSamples)/2)\n",
    "    for (Lines, _, Gender) in rawData[:trainingSamples] + rawData[halfOfData:halfOfData+trainingSamples]: #train data percentage based on split\n",
    "        trainData.append((toFeatureVector(preProcess(Lines),spacystring(Lines)),Gender))\n",
    "#     for (Lines, _, Gender) in rawData[trainingSamples:halfOfData] + rawData[halfOfData+trainingSamples:]: #uncomment to test on validation set\n",
    "#         testData.append((toFeatureVector(preProcess(Lines),spacystring(Lines)),Gender))\n",
    "    for (Lines, _, Gender) in rawTest:\n",
    "        testData.append((toFeatureVector(preProcess(Lines),spacystring(Lines)),Gender)) #Process the test data and append them to testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert line from input file into an id/text/label tuple\n",
    "def parseReview(line):\n",
    "    # Should return a tuple of an integer, a string containing the review, and a string indicating the label \n",
    "    \n",
    "    Lines = str(line[0])\n",
    "    Char = str(line[1])\n",
    "    Gender = str(line[2])\n",
    "    \n",
    " \n",
    "    # DESCRIBE YOUR METHOD IN WORDS\n",
    "    #Feeding lines of data into its corresponding object name and returning it to later \n",
    "    #be called in the loadData function to be appended to rawData list\n",
    "    \n",
    "    return (Lines, Char, Gender) #returning these 3 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcess(Text):\n",
    "# Should return a list of tokens\n",
    "   # string.punctuation\n",
    "\n",
    "    #print(\"original:\", Text)\n",
    "\n",
    "    # word tokenisation\n",
    "    Text = Text.strip(string.punctuation)\n",
    "    Text = re.sub(r\"(\\w)([.,;:!?'\\\"”\\)])\", r\"\\1 \\2\", Text) #separating words and special charaters from words that comes after the word\n",
    "    Text = re.sub(r\"([.,;:!?'\\\"“\\(])(\\w)\", r\"\\1 \\2\", Text) #separating words and special charaters from words that comes before the word\n",
    "    Text =  re.sub(r'[^\\w\\s]','',Text)\n",
    "    Text = re.sub(r'\\_','',Text)\n",
    "#    Text =  \"\".join([w for w in Text2 if w not in string.punctuation])\n",
    "\n",
    "    tokens = re.split(r\"\\s+\",Text)   #tokenize\n",
    "    # normalisation\n",
    "    tokens = [t.lower() for t in tokens]   #lower casing the words\n",
    "    \n",
    "#     # stopword removal\n",
    "#     stop = set(stopwords.words('english'))  #instantiate stop object that contains all the stopwords\n",
    "#     tokens = [t for t in tokens if t not in stop]  #removes stopwords from sentence\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should return a dictionary containing features as keys, and weights as values\n",
    "\n",
    "featureDict = {} # A global dictionary of features\n",
    "emptyline = \"\"   # object of empty string to check for empty lines\n",
    "empty = \"Empty\"  # object with the string 'empty'\n",
    "            \n",
    "posttagger = CRFTagger()\n",
    "posttagger.set_model_file(\"crf_pos.tagger\")\n",
    "\n",
    "\n",
    "\n",
    "def toFeatureVector(tokens,spacytok):\n",
    "    tagged = posttagger.tag(tokens)        #instatiate the postagger\n",
    "    localFeat = {}                         #Local dictionary to be returned\n",
    "\n",
    "    \n",
    "    if not tokens[0]==False and (tokens[0].strip() == emptyline):  #Checks if the first element of the token is empty\n",
    "        localFeat[empty] = 1           #Adds the word \"Empty\" and weight 1 if its empty, else continue as usual.\n",
    "        \n",
    "    else:\n",
    "######### 1. Weighted word count for all words in sentence ##########\n",
    "#####################################################################\n",
    "        for words in tokens:\n",
    "            try:\n",
    "                i = featureDict[words] #Tries adding the weights to existing tokens that's been calculated\n",
    "            except KeyError:\n",
    "                i = len(featureDict) + 1  #adds 1 to length of feature dict\n",
    "                featureDict[words] = i #adds the word and its number of frequency\n",
    "            try:\n",
    "                localFeat[words] += (1.0/len(tokens)) #Tries adding the weights to existing tokens that's been calculated\n",
    "            except KeyError:\n",
    "                localFeat[words] = (1.0/len(tokens)) #If error, code will exucute this line and add the word to dictionary\n",
    "        \n",
    "######### 2. Adding postags as keys and the counts as weights #########\n",
    "#######################################################################\n",
    "\n",
    "#         for word in tagged:\n",
    "#             try:\n",
    "#                 localFeat[word[1]] += (1.0/len(tagged)) #Tries adding the weights to existing postags. \n",
    "#             except KeyError:\n",
    "#                 localFeat[word[1]] = (1.0/len(tagged)) #If the tag is not present in dictionary, this line will add to it along with its weight.\n",
    "\n",
    "######### 3. Adding word as keys and postags data #####################\n",
    "#######################################################################\n",
    "\n",
    "#         for word in tagged:           \n",
    "#             localFeat[word[0]] = (word[1]) #Adding word as keys and postags data\n",
    "\n",
    "######### 4. Adding NER (BIO tags) as keys and the counts as weights ##\n",
    "#######################################################################\n",
    "\n",
    "#         for w in spacytok:           \n",
    "#             ent = [w.ent_iob_]  #extracts the labeled entity of the token\n",
    "#             try:\n",
    "#                 localFeat[ent[0]] += (1.0/len(spacytok)) #Tries adding the weights to existing BIO tags\n",
    "#             except KeyError:\n",
    "#                 localFeat[ent[0]] = (1.0/len(spacytok)) #adds the bio tag and its weight if its not there already\n",
    "#             try:\n",
    "#                 i = featureDict[ent[0]] #Tries adding the weights to existing tokens that's been calculated\n",
    "#             except KeyError:\n",
    "#                 i = len(featureDict) + 1  #adds 1 to length of feature dict\n",
    "#                 featureDict[ent[0]] = i #adds the word and its number of frequency\n",
    "\n",
    "\n",
    "########## 5. Adding word as keys and NER (BIO tags) as data ##########\n",
    "#######################################################################\n",
    "\n",
    "#         for w in spacytok:           \n",
    "#             ent = [w.ent_iob_]  #extracts the labeled entity of the token         \n",
    "#             localFeat[w] = (ent[0]) #adds the bio tag and its weight if its not there already\n",
    "          \n",
    "                \n",
    "########## 6. Adding dependency (dep) as keys and freq as weight ######\n",
    "#######################################################################\n",
    "\n",
    "#         for d in spacytok:\n",
    "            \n",
    "#             ent = [d.dep_]  #extracts the labeled entity of the token\n",
    "#             try:\n",
    "#                 localFeat[ent[0]] += (1.0/len(spacytok)) #Tries adding the weights to existing BIO tags\n",
    "#             except KeyError:\n",
    "#                 localFeat[ent[0]] = (1.0/len(spacytok)) #adds the bio tag and its weight if its not there already\n",
    "#             try:\n",
    "#                 i = featureDict[ent[0]] #Tries adding the weights to existing tokens that's been calculated\n",
    "#             except KeyError:\n",
    "#                 i = len(featureDict) + 1  #adds 1 to length of feature dict\n",
    "#                 featureDict[ent[0]] = i #adds the word and its number of frequency\n",
    " \n",
    "    \n",
    "########## 7. Adding word as keys and dependency (dep) as data ########\n",
    "#######################################################################\n",
    "\n",
    "#         for d in spacytok:\n",
    "#             ent = [d.dep_]     #extracts the dependency tag         \n",
    "#             localFeat[d] = (ent[0]) #adds word as keys and dependency (dep) as data\n",
    "\n",
    "\n",
    "    return (localFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainClassifier(trainData):\n",
    "    print(\"Training Classifier...\")\n",
    "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
    "    #pipeline =  Pipeline([('clf', MultinomialNB())])\n",
    "     \n",
    "    return SklearnClassifier(pipeline).train(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictLabels(reviewSamples, classifier):\n",
    "    return classifier.classify_many(map(lambda t: t[0], reviewSamples))\n",
    "\n",
    "def predictLabel(reviewSample, classifier):\n",
    "    return classifier.classify(toFeatureVector(preProcess(reviewSample)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "precisionAvg = []\n",
    "recallAvg = []\n",
    "fscoreAvg = []\n",
    "\n",
    "\n",
    "def crossValidate(dataset, folds):\n",
    "    shuffle(dataset)                 # shuffles the dataset\n",
    "    onefold = int(len(dataset)/folds) # Calculating the length for one fold\n",
    "\n",
    "\n",
    "    fold1 = dataset[0:onefold]           # splitting the dataset according to the size of folds. In this case 10 folds.\n",
    "    fold2 = dataset[onefold:onefold*2]\n",
    "    fold3 = dataset[onefold*2:onefold*3]\n",
    "    fold4 = dataset[onefold*3:onefold*4]\n",
    "    fold5 = dataset[onefold*4:onefold*5]\n",
    "    fold6 = dataset[onefold*5:onefold*6]\n",
    "    fold7 = dataset[onefold*6:onefold*7]\n",
    "    fold8 = dataset[onefold*7:onefold*8]\n",
    "    fold9 = dataset[onefold*8:onefold*9]\n",
    "    fold10 = dataset[onefold*9:]\n",
    "\n",
    "    #The following part maps the fold sections to 10 different crossvalidation iteration, where each iteration,\n",
    "    #the test fold changes.\n",
    "\n",
    "    \n",
    "    cvfoldtest[0] = fold1\n",
    "    cvfoldtrain[0] = fold2 + fold3+fold4+ fold5+ fold6+fold7+ fold8+ fold9+ fold10\n",
    "\n",
    "    cvfoldtest[1] = fold2\n",
    "    cvfoldtrain[1] = fold1 + fold3+fold4+ fold5+ fold6+fold7+ fold8+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[2] = fold3\n",
    "    cvfoldtrain[2] = fold1 + fold2+fold4+ fold5+ fold6+fold7+ fold8+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[3] = fold4\n",
    "    cvfoldtrain[3] = fold1 + fold2+fold3+ fold5+ fold6+fold7+ fold8+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[4] = fold5\n",
    "    cvfoldtrain[4] = fold1 + fold2+fold3+ fold4+ fold6+fold7+ fold8+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[5] = fold6\n",
    "    cvfoldtrain[5] = fold1 + fold2+fold3+ fold4+ fold5+fold7+ fold8+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[6] = fold7\n",
    "    cvfoldtrain[6] = fold1 + fold2+fold3+ fold4+ fold5+fold6+ fold8+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[7] = fold8\n",
    "    cvfoldtrain[7] = fold1 + fold2+fold3+ fold4+ fold5+fold6+ fold7+ fold9+ fold10 \n",
    "\n",
    "    cvfoldtest[8] = fold9\n",
    "    cvfoldtrain[8] = fold1 + fold2+fold3+ fold4+ fold5+fold6+ fold7+ fold8+ fold10 \n",
    "\n",
    "    cvfoldtest[9] = fold10\n",
    "    cvfoldtrain[9] = fold1 + fold2+fold3+ fold4+ fold5+fold6+ fold7+ fold8+ fold9\n",
    "\n",
    "    for i in range(folds):   \n",
    "     \n",
    "        classifier = trainClassifier(cvfoldtrain[i])        # train the classifier\n",
    "        testTrue = [t[1] for t in cvfoldtest[i]]                  # get the ground-truth labels from the data\n",
    "        testPred = predictLabels(cvfoldtest[i], classifier)\n",
    "        finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted')\n",
    "        print(\"Crossvalidation iteration number: \", i+1)\n",
    "        print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])\n",
    "    \n",
    "        precisionAvg.append(finalScores[0])  #appends data to object for average calculation\n",
    "        recallAvg.append(finalScores[1])\n",
    "        fscoreAvg.append(finalScores[2])\n",
    "    \n",
    "    finalPrecisionAvg = sum(precisionAvg)/10\n",
    "    finalRecallAvg = sum(recallAvg)/10\n",
    "    finalFscoreAvg = sum(fscoreAvg)/10\n",
    "    \n",
    "    print(\"10 fold crossvalidation results: \")\n",
    "    print (\"The average precision score is \", finalPrecisionAvg ) #Result for precision average \n",
    "    print  (\"The average recall score is \", finalRecallAvg ) #Result for recall average\n",
    "    print (\"The average fscore score is \", finalFscoreAvg ) #Result for fscore average\n",
    "    \n",
    "    return finalPrecisionAvg, finalRecallAvg, finalFscoreAvg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now 0 rawData, 0 trainData, 0 testData\n",
      "Preparing the dataset...\n",
      "Now 10113 rawData, 0 trainData, 0 testData\n",
      "Preparing training and test data...\n",
      "After split, 10113 rawData, 10112 trainData, 1124 testData\n",
      "Training Samples: \n",
      "10112\n",
      "Features: \n",
      "5906\n",
      "length train 10112\n",
      "length test 1124\n"
     ]
    }
   ],
   "source": [
    "##### Main ####\n",
    "\n",
    "rawData = []          # the filtered data from the dataset file (should be 21000 samples)\n",
    "rawTest = []          # Raw test data\n",
    "trainData = []        # the pre-processed training data as a percentage of the total dataset (currently 80%, or 16800 samples)\n",
    "testData = []         # the pre-processed test data as a percentage of the total dataset (currently 20%, or 4200 samples)\n",
    "\n",
    "\n",
    "# references to the data files\n",
    "reviewPath = 'training.csv'\n",
    "rawtestset = 'test.csv'\n",
    "# Do the actual stuff (i.e. call the functions we've made)\n",
    "# We parse the dataset and put it in a raw data list\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing the dataset...\",sep='\\n')\n",
    "loadData(reviewPath) \n",
    "loadTest(rawtestset)\n",
    "\n",
    "# We split the raw dataset into a set of training data and a set of test data (80/20)\n",
    "# You do the cross validation on the 80% (training data)\n",
    "# We print the number of training samples and the number of features before the split\n",
    "print(\"Now %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Preparing training and test data...\",sep='\\n')\n",
    "splitData(1)\n",
    "# We print the number of training samples and the number of features after the split\n",
    "print(\"After split, %d rawData, %d trainData, %d testData\" % (len(rawData), len(trainData), len(testData)),\n",
    "      \"Training Samples: \", len(trainData), \"Features: \", len(featureDict), sep='\\n')\n",
    "\n",
    "print('length train', len(trainData))\n",
    "print('length test', len(testData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "onefold = int(len(trainData)/10)\n",
    "fold1 = trainData[0:onefold]\n",
    "fold2 = trainData[onefold:onefold*2]\n",
    "fold3 = trainData[onefold*2:onefold*3]\n",
    "fold4 = trainData[onefold*3:onefold*4]\n",
    "fold5 = trainData[onefold*4:onefold*5]\n",
    "fold6 = trainData[onefold*5:onefold*6]\n",
    "fold7 = trainData[onefold*6:onefold*7]\n",
    "fold8 = trainData[onefold*7:onefold*8]\n",
    "fold9 = trainData[onefold*8:onefold*9]\n",
    "fold10 = trainData[onefold*9:]\n",
    "\n",
    "#print(fold1)\n",
    "cvfoldtest = fold1\n",
    "cvfoldtrain = fold2 + fold3+fold4+ fold5+ fold6+fold7+ fold8+ fold9+ fold10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## 10-Fold Cross-validation ###############\n",
    "\n",
    "# cvResults = crossValidate(trainData, 10) #calling CV function\n",
    "# print(cvResults) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Classifier...\n",
      "Done training!\n",
      "Precision: 0.588599\n",
      "Recall: 0.587189\n",
      "F Score:0.587580\n"
     ]
    }
   ],
   "source": [
    "functions_complete = True  # set to True once you're happy with your methods for cross val\n",
    "if functions_complete:\n",
    "    #print(testData[0])   # have a look at the first test data instance\n",
    "    classifier = trainClassifier(trainData)  # train the classifier\n",
    "    testTrue = [t[1] for t in testData]   # get the ground-truth labels from the data\n",
    "    testPred = predictLabels(testData, classifier)  # classify the test data to get predicted labels~\n",
    "    finalScores = precision_recall_fscore_support(testTrue, testPred, average='weighted') # evaluate\n",
    "    print(\"Done training!\")\n",
    "    print(\"Precision: %f\\nRecall: %f\\nF Score:%f\" % finalScores[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female       0.56      0.58      0.57       526\n",
      "        male       0.62      0.59      0.60       598\n",
      "\n",
      "    accuracy                           0.59      1124\n",
      "   macro avg       0.59      0.59      0.59      1124\n",
      "weighted avg       0.59      0.59      0.59      1124\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classReport= classification_report(testTrue, testPred)\n",
    "print(classReport)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results (Fine tune on validation set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 1. Classification report for using weighted word count (Benchmark) #######\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.55      0.53      0.54      1017\n",
    "#         male       0.54      0.56      0.55      1006\n",
    "\n",
    "#     accuracy                           0.55      2023\n",
    "#    macro avg       0.55      0.55      0.55      2023\n",
    "# weighted avg       0.55      0.55      0.55      2023\n",
    "\n",
    "##Comments\n",
    "##As a benchmark, I'm using a simple a weighted word count for in the toFeatureVector as I did in assignment 1.\n",
    "##The result is decent, with a macro average of 55.\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.5634858116939093\n",
    "# The average recall score is  0.5625463535228677\n",
    "# The average fscore score is  0.5620891366442106\n",
    "\n",
    "##Comments\n",
    "##Using crossvalidation on the same feature vector produce a result of 0.562\n",
    "##The avg fscore increases after using crossvalidation on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# 2. Classification report for using Postags #######################\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.52      0.50      0.51      1017\n",
    "#         male       0.51      0.54      0.53      1006\n",
    "\n",
    "#     accuracy                           0.52      2023\n",
    "#    macro avg       0.52      0.52      0.52      2023\n",
    "# weighted avg       0.52      0.52      0.52      2023\n",
    "\n",
    "##Comments\n",
    "##Using the postags alone in the toFeatureVector function reduces the macro avg to 52\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.5184037732730571\n",
    "# The average recall score is  0.5165636588380718\n",
    "# The average fscore score is  0.5152655555112817\n",
    "\n",
    "##Comments\n",
    "##Using crossvalidation on the same feature vector produce a result of 0.515\n",
    "##The avg fscore remains unchanged even after using crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 3. Classification report for weighted word count and Postags ###########\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.55      0.54      0.54      1017\n",
    "#         male       0.54      0.56      0.55      1006\n",
    "\n",
    "#     accuracy                           0.55      2023\n",
    "#    macro avg       0.55      0.55      0.55      2023\n",
    "# weighted avg       0.55      0.55      0.55      2023\n",
    "\n",
    "##Comments\n",
    "##Combining the postags and the weighted word counts doesn't improve the macro avg.\n",
    "##The result remains at 55 using this method.\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.5629531756078207\n",
    "# The average recall score is  0.5610630407911001\n",
    "# The average fscore score is  0.560739974495919\n",
    "\n",
    "##Comments\n",
    "##Using crossvalidation on the same feature vector produces a result of 0.56\n",
    "##The avg fscore increased after using crossvalidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 4. Classification report for NER (BIO-tags) as features ################\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.50      0.16      0.24      1017\n",
    "#         male       0.50      0.84      0.62      1006\n",
    "\n",
    "#     accuracy                           0.50      2023\n",
    "#    macro avg       0.50      0.50      0.43      2023\n",
    "# weighted avg       0.50      0.50      0.43      2023\n",
    "\n",
    "##Comments\n",
    "##By using the bio tags as weighted features, the macro avg is 0.43\n",
    "##The tags on its own is not capable of improving the model\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.5141275912199073\n",
    "# The average recall score is  0.5076637824474659\n",
    "# The average fscore score is  0.4440490339322534\n",
    "\n",
    "##Comments\n",
    "##However, by using crossvalidation, it did increase the performance by 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 5. Classification report for dependency features as keys and weight ####\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.51      0.45      0.48      1017\n",
    "#         male       0.50      0.55      0.53      1006\n",
    "\n",
    "#     accuracy                           0.50      2023\n",
    "#    macro avg       0.50      0.50      0.50      2023\n",
    "# weighted avg       0.50      0.50      0.50      2023\n",
    "\n",
    "##Comments\n",
    "##By using syntactic dependency as features, the macro avg 0.50\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.502701695421722\n",
    "# The average recall score is  0.5006180469715698\n",
    "# The average fscore score is  0.498880598781958\n",
    "\n",
    "##Comments\n",
    "##The performance of the model using crossvalidation didnt increae either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 6. Classification report for dependency (word as keys and dep as data) #\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.54      0.58      0.56      1017\n",
    "#         male       0.54      0.50      0.52      1006\n",
    "\n",
    "#     accuracy                           0.54      2023\n",
    "#    macro avg       0.54      0.54      0.54      2023\n",
    "# weighted avg       0.54      0.54      0.54      2023\n",
    "\n",
    "##Comments\n",
    "##Using syntactic dependency alone, the model managed to get a macro avg of 0.54\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.5482369429454279\n",
    "# The average recall score is  0.5474660074165636\n",
    "# The average fscore score is  0.5474281453967389\n",
    "\n",
    "##Comments\n",
    "##Cross validation shows that across every slice, the model managed to obtain\n",
    "##a result of of almost 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### 7. Combining Pos tags and Dependency   #########################\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.55      0.57      0.56      1017\n",
    "#         male       0.55      0.53      0.54      1006\n",
    "\n",
    "#     accuracy                           0.55      2023\n",
    "#    macro avg       0.55      0.55      0.55      2023\n",
    "# weighted avg       0.55      0.55      0.55      2023\n",
    "\n",
    "##Comments\n",
    "##By combining postags and dependency tags as features, the macro avg is 0.55\n",
    "\n",
    "####################### Cross validation result ################################\n",
    "################################################################################\n",
    "\n",
    "# 10 fold crossvalidation results: \n",
    "# The average precision score is  0.5432027884531633\n",
    "# The average recall score is  0.5428924598269468\n",
    "# The average fscore score is  0.5426211409428975\n",
    "\n",
    "##Comments\n",
    "##However, the crossvalidation achieved an avg of 0.54 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results (on test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 3. Classification report for weighted word count and Postags ###########\n",
    "################################################################################\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#       female       0.56      0.58      0.57       526\n",
    "#         male       0.62      0.59      0.60       598\n",
    "\n",
    "#     accuracy                           0.59      1124\n",
    "#    macro avg       0.59      0.59      0.59      1124\n",
    "# weighted avg       0.59      0.59      0.59      1124\n",
    "\n",
    "################ precision_recall_fscore_support ###############################\n",
    "\n",
    "# Precision: 0.588599\n",
    "# Recall: 0.587189\n",
    "# F Score:0.587580\n",
    "\n",
    "## After testing different combinations of features (postags, biotags, dependency, word count), the best combination for me\n",
    "## was the word count and postags together.\n",
    "\n",
    "## Using the best combination of features (weighted word count and Postags) which obtained 0.56 on the validation set,\n",
    "## it managed to obtain 0.59 on the test set.\n",
    "\n",
    "## for a more precise number, the precision_recall_fscore_support function outputs the result to 6 decimal places.\n",
    "## The f-score is actually 0.587\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
